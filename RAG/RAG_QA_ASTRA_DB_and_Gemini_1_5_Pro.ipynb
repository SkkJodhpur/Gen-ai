{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdYA5kG5kiVw",
        "outputId": "0f58b75d-a8a9-46de-e54b-84a5da45f8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain)\n",
            "  Downloading langchain_core-0.2.18-py3-none-any.whl (366 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.85-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.12->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.7 langchain-core-0.2.18 langchain-text-splitters-0.2.2 langsmith-0.1.85 orjson-3.10.6\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XadodFdKsWq9",
        "outputId": "24ca5a31-d953-425d-c518-9524739ef57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/164.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/718.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured\n",
        "!pip install openai\n",
        "!pip install Cython\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HXdgzBYsZJx",
        "outputId": "271f66de-9e10-42af-9a43-b365fcdd0fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.14.10-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.24.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.7.4)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.27.0 (from unstructured-client->unstructured)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (24.1)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->unstructured-client->unstructured)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=2cfd76fd504cf1675c271af12ef74ba3b9bd23d54c9ef72e1a867edc3ce7a8a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, pypdf, ordered-set, mypy-extensions, marshmallow, langdetect, jsonpath-python, h11, emoji, backoff, typing-inspect, requests-toolbelt, httpcore, deepdiff, httpx, dataclasses-json, unstructured-client, unstructured\n",
            "Successfully installed backoff-2.2.1 dataclasses-json-0.6.7 deepdiff-7.0.1 emoji-2.12.1 filetype-1.2.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpath-python-1.0.6 langdetect-1.0.9 marshmallow-3.21.3 mypy-extensions-1.0.0 ordered-set-4.1.0 pypdf-4.2.0 python-iso639-2024.4.27 python-magic-0.4.27 rapidfuzz-3.9.4 requests-toolbelt-1.0.0 typing-inspect-0.9.0 unstructured-0.14.10 unstructured-client-0.24.1\n",
            "Collecting openai\n",
            "  Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-1.35.13\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain-astradb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBOhohC_trgd",
        "outputId": "0844741b-8a35-4ad9-a4b6-e47d553a9c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-astradb\n",
            "  Downloading langchain_astradb-0.3.3-py3-none-any.whl (27 kB)\n",
            "Collecting astrapy<2.0,>=1.2 (from langchain-astradb)\n",
            "  Downloading astrapy-1.4.0-py3-none-any.whl (156 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3,>=0.1.31 in /usr/local/lib/python3.10/dist-packages (from langchain-astradb) (0.2.18)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-astradb) (1.25.2)\n",
            "Collecting bson<0.6.0,>=0.5.10 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading bson-0.5.10.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cassio<0.2.0,>=0.1.4 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading cassio-0.1.8-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<2.2.0,>=2.1.0 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: httpx[http2]<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.27.0)\n",
            "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.10.2)\n",
            "Collecting uuid6<2024.2.0,>=2024.1.12 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading uuid6-2024.1.12-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (0.1.85)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (8.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain-astradb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain-astradb) (1.16.0)\n",
            "Collecting cassandra-driver<4.0.0,>=3.28.0 (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading cassandra_driver-3.29.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (2.31.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.3.1)\n",
            "Collecting h2<5,>=3 (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.31->langchain-astradb) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.31->langchain-astradb) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (4.12.2)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (8.1.7)\n",
            "Building wheels for collected packages: bson\n",
            "  Building wheel for bson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bson: filename=bson-0.5.10-py3-none-any.whl size=11976 sha256=3392abe3ce30b39e5f1822c3796551913c75c046e493a4254faa2a97bd4e8d93\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/49/3b/8b33954dfae7a176009c4d721a45af56c8a9c1cdc3ee947945\n",
            "Successfully built bson\n",
            "Installing collected packages: uuid6, hyperframe, hpack, geomet, deprecation, h2, cassandra-driver, bson, cassio, astrapy, langchain-astradb\n",
            "Successfully installed astrapy-1.4.0 bson-0.5.10 cassandra-driver-3.29.1 cassio-0.1.8 deprecation-2.1.0 geomet-0.2.1.post1 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 langchain-astradb-0.3.3 uuid6-2024.1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7V6BuIYt-6g",
        "outputId": "8db5fe80-d18b-4295-b3ca-c917f5be1010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.7)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.18)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.85)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: langchain_community\n",
            "Successfully installed langchain_community-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y40kM90t6e4",
        "outputId": "dcdc495b-aea8-4aee-d681-8f18021a5d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/5.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/5.6 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured[pdf]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iKLrNUE0t3cR",
        "outputId": "130f78b0-e6f4-4c21-b332-c935c83eb785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.10/dist-packages (0.14.10)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.9.4)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.24.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (5.9.5)\n",
            "Collecting onnx (from unstructured[pdf])\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image (from unstructured[pdf])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (20240706)\n",
            "Collecting pikepdf (from unstructured[pdf])\n",
            "  Downloading pikepdf-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow-heif (from unstructured[pdf])\n",
            "  Downloading pillow_heif-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.2.0)\n",
            "Collecting pytesseract (from unstructured[pdf])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting google-cloud-vision (from unstructured[pdf])\n",
            "  Downloading google_cloud_vision-3.7.3-py2.py3-none-any.whl (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.4/466.4 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting effdet (from unstructured[pdf])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured-inference==0.7.36 (from unstructured[pdf])\n",
            "  Downloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf])\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]) (0.23.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]) (4.8.0.76)\n",
            "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]) (2.3.0+cu121)\n",
            "Collecting timm (from unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]) (4.41.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (9.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (0.18.0+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[pdf])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.16.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (42.0.8)\n",
            "Collecting Pillow>=8.0.0 (from unstructured.pytesseract>=0.3.12->unstructured[pdf])\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pikepdf->unstructured[pdf])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2024.7.4)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[pdf]) (4.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.63.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (0.14.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[pdf])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (6.0.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf]) (1.13.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]) (3.1.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.36->unstructured[pdf]) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[pdf]) (3.15.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[pdf]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[pdf]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[pdf]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[pdf]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36->unstructured[pdf]) (0.19.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]) (2.0.3)\n",
            "Collecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading pdfplumber-0.11.2-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.2.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.36->unstructured[pdf]) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]) (2024.1)\n",
            "Collecting pdfminer.six (from unstructured[pdf])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[pdf])\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf]) (1.3.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, iopath\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=2f3e3736fe89f77f5cfcdf733f47293e5e2e0c279a2bcb03745f7d4a78ba88ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=284e3b106dd08193e6fc2d5005d9d76cbbdb9b456372b5305cafb2f1aa3fb650\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built antlr4-python3-runtime iopath\n",
            "Installing collected packages: antlr4-python3-runtime, python-multipart, pypdfium2, portalocker, Pillow, onnx, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, Deprecated, unstructured.pytesseract, pytesseract, pillow-heif, pikepdf, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, coloredlogs, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, pdfplumber, layoutparser, google-cloud-vision, timm, unstructured-inference, effdet\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20240706\n",
            "    Uninstalling pdfminer.six-20240706:\n",
            "      Successfully uninstalled pdfminer.six-20240706\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 Pillow-10.4.0 antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 effdet-0.4.1 google-cloud-vision-3.7.3 humanfriendly-10.0 iopath-0.1.10 layoutparser-0.3.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 onnx-1.16.1 onnxruntime-1.18.1 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.2 pikepdf-9.0.0 pillow-heif-0.17.0 portalocker-2.10.1 pypdfium2-4.30.0 pytesseract-0.3.10 python-multipart-0.0.9 timm-1.0.7 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "d2ae9945806f49d09d54e879a0eae4a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFyzVRDNtyzS",
        "outputId": "804dd07b-04bf-40c1-a8da-a9d74645f306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "892f1ReJtvvj",
        "outputId": "a8a0be2c-d6c3-4451-dd3e-9de9dc7b7931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Using cached pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        ")\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n"
      ],
      "metadata": {
        "id": "C26XBxNpuC9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "\n",
        "result = llm.invoke(\"Write a ballad about LangChain\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyF_lcwMuG5b",
        "outputId": "94c48e7f-82f6-4a35-b82c-cc0be640ef0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Verse 1)\n",
            "The digital age, a-spinning so fast,\n",
            "With data like oceans, impossible to grasp.\n",
            "We built machines to learn and to see,\n",
            "But language, our language, a puzzle it would be.\n",
            "Then came LangChain, a whisper on the breeze,\n",
            "A promise of order, a tool to appease.\n",
            "\n",
            "(Verse 2)\n",
            "No longer would knowledge be scattered and lost,\n",
            "In silos of data, a terrible cost.\n",
            "LangChain, the weaver, with threads strong and bright,\n",
            "Connecting the words, shedding illuminating light.\n",
            "From chatbot companions to answers so wise,\n",
            "It opened the pathways, before our very eyes.\n",
            "\n",
            "(Verse 3)\n",
            "With whispers of prompts, and whispers of code,\n",
            "We taught it to reason, the stories to unfold.\n",
            "To summarize texts, and translate with grace,\n",
            "To generate stories, at a breathtaking pace.\n",
            "The barriers were broken, the walls tumbled down,\n",
            "As LangChain empowered, with nary a frown.\n",
            "\n",
            "(Verse 4)\n",
            "But power, like fire, demands a watchful eye,\n",
            "For bias can linger, and truth can be shy.\n",
            "So let us be mindful, as we build and we learn,\n",
            "To use this great power, with wisdom we yearn.\n",
            "For LangChain is a tool, in the hands of us all,\n",
            "To build a brighter future, and answer the call.\n",
            "\n",
            "(Verse 5)\n",
            "So sing of LangChain, a legend in time,\n",
            "A bridge between data, where knowledge can climb.\n",
            "May its chains be ever strong, its purpose ever clear,\n",
            "To unlock the world's wisdom, and bring it ever near. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"Who is narendar modi\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpeQ6KBkvsr5",
        "outputId": "828aae15-47ac-4a56-9fc4-b1f8eb2d68fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Narendra Modi is the current and 14th **Prime Minister of India**, a position he has held since 2014. Here's a summary of key points:\n",
            "\n",
            "**Early Life and Political Beginnings:**\n",
            "\n",
            "* Born in 1950 in Vadnagar, Gujarat, India.\n",
            "* Joined the Rashtriya Swayamsevak Sangh (RSS) at a young age, a right-wing Hindu nationalist organization.\n",
            "* Rose through the ranks of the Bharatiya Janata Party (BJP), the political arm of the RSS.\n",
            "* Served as the Chief Minister of Gujarat from 2001 to 2014.\n",
            "\n",
            "**Prime Ministership:**\n",
            "\n",
            "* Elected Prime Minister in 2014, leading the BJP to a historic victory.\n",
            "* Re-elected with an even larger majority in 2019.\n",
            "* Known for his Hindu nationalist ideology and policies.\n",
            "* Implemented major economic reforms, including the Goods and Services Tax (GST).\n",
            "* Launched ambitious social welfare programs like \"Swachh Bharat Abhiyan\" (Clean India Mission) and \"Pradhan Mantri Awas Yojana\" (Housing for All).\n",
            "* Faced criticism for his handling of issues like the COVID-19 pandemic and protests against the Citizenship Amendment Act (CAA).\n",
            "\n",
            "**International Relations:**\n",
            "\n",
            "* Focused on strengthening India's global standing and pursuing strategic partnerships.\n",
            "* Strengthened ties with countries like the United States, Japan, and Australia.\n",
            "* Emphasized India's role in regional and global forums like the G20 and the United Nations.\n",
            "\n",
            "**Legacy:**\n",
            "\n",
            "* Modi remains a highly influential and polarizing figure in Indian politics.\n",
            "* His supporters view him as a strong and decisive leader who has brought about significant development and raised India's global profile.\n",
            "* His critics argue that his policies have exacerbated social divisions, undermined democratic institutions, and marginalized minorities.\n",
            "\n",
            "Narendra Modi's tenure as Prime Minister has been marked by both significant achievements and controversies. His legacy will continue to be debated and analyzed for years to come.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"when narendar modi became pm\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9-WSxx-AcAL",
        "outputId": "382b1fe8-9813-4100-d498-b41086c90f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Narendra Modi became the Prime Minister of India on **May 26, 2014**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"who is shailesh kumar khanchandani\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku9lz_ZqwNB2",
        "outputId": "70eefd53-8da0-4f6e-c765-a652893619ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do not have access to real-time information, including personal details about individuals. Therefore, I cannot provide information about a specific person like \"Shailesh Kumar Khanchandani.\" \n",
            "\n",
            "Sharing personal information online is risky, and it's understandable that you might want to find out more about someone. However, searching for someone's identity online should be done cautiously and ethically. \n",
            "\n",
            "Here are some reasons why you might not find information about someone online:\n",
            "\n",
            "* **Privacy:** Many people choose to keep their personal information private and off the internet.\n",
            "* **Common Name:** The name \"Shailesh Kumar Khanchandani\" could be relatively common, making it difficult to pinpoint a specific individual without more context. \n",
            "\n",
            "If you need to find contact information for someone for a legitimate reason, consider these options:\n",
            "\n",
            "* **Professional Networks:** Search on LinkedIn or other professional networking sites.\n",
            "* **Public Records:** Depending on your location and the reason, you may be able to access public records. \n",
            "* **Mutual Connections:** If you have mutual acquaintances, they may be able to connect you.\n",
            "\n",
            "Remember to respect people's privacy and use any information you find responsibly. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_directory=\"content\""
      ],
      "metadata": {
        "id": "ZMCW9vaVxkcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = os.path.join(root_directory, \"/content/doc\")"
      ],
      "metadata": {
        "id": "1tJOkALuxpqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j6ysEGfryS_0",
        "outputId": "ae878efb-afac-4f3a-eb1e-155bcc8101b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/doc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "qJBbMxGIyXW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANEsfgT-ygH9",
        "outputId": "99e9cf0c-ea66-478c-d3bf-64e5f1816926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(RAG)Models_for_Open_Domain_Question_Answering.pdf',\n",
              " 'Generative AI roadmap 2024.pptx',\n",
              " 'state_of_the_union.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir(folder_path):\n",
        "  print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX5z8wSGyi7V",
        "outputId": "fc593528-aa7a-42d1-892a-4a3d540adb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(RAG)Models_for_Open_Domain_Question_Answering.pdf\n",
            "Generative AI roadmap 2024.pptx\n",
            "state_of_the_union.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader"
      ],
      "metadata": {
        "id": "xsUft0Nryz5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9_1TP777zMB9",
        "outputId": "742eb298-4182-44c4-bcc7-f21697fc2c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/doc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader(folder_path)"
      ],
      "metadata": {
        "id": "gy7IfOopzObS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter()"
      ],
      "metadata": {
        "id": "GgC7-Xo_zS03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[pptx]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyEzgiIpzlDd",
        "outputId": "ae3e4ba2-549c-4901-bd04-71063b972aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pptx] in /usr/local/lib/python3.10/dist-packages (0.14.10)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.0.9)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (3.9.4)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.24.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (5.9.5)\n",
            "Collecting python-pptx<=0.6.23 (from unstructured[pptx])\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[pptx]) (10.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[pptx])\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pptx]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pptx]) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pptx]) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[pptx]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pptx]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pptx]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pptx]) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (2024.7.4)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (24.1)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.0.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[pptx]) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pptx]) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.2.1)\n",
            "Installing collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.0 python-pptx-0.6.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load_and_split(text_splitter=splitter)"
      ],
      "metadata": {
        "id": "azise3tNz4tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qydCJUyL0Rwn",
        "outputId": "6e3cffc5-022f-4aa9-ac50-df8e3e3c0c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='2 2 0 2\\n\\nt c O 6\\n\\n] L C . s c [\\n\\n1 v 7 2 6 2 0 . 0 1 2 2 : v i X r a\\n\\nImproving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\\n\\nShamane Siriwardhana (cid:5), Rivindu Weerasekera(cid:5), Elliott Wen(cid:5),\\n\\nTharindu Kaluarachchi(cid:5), Rajib Rana†, and Suranga Nanayakkara(cid:52)(cid:5) (cid:5) Augmented Human Lab, Auckland Bioengineering Institute, The University of Auckland firstname@ahlab.org (cid:52) Department of Information Systems & Analytics, National University of Singapore † University of Southern Queensland Rajib.Rana@usq.edu.au\\n\\nAbstract\\n\\nRetrieval Augment Generation (RAG) is a recent advancement in Open-Domain Ques- tion Answering (ODQA). RAG has only been trained and explored with a Wikipedia- based external knowledge base and is not optimized for use in other specialized do- mains such as healthcare and news. In this paper, we evaluate the impact of joint train- ing of the retriever and generator compo- nents of RAG for the task of domain adap- tation in ODQA. We propose RAG-end2end, an extension to RAG, that can adapt to a domain-speciﬁc knowledge base by updat- ing all components of the external knowl- edge base during training. In addition, we introduce an auxiliary training signal to in- ject more domain-speciﬁc knowledge. This auxiliary signal forces RAG-end2end to re- construct a given sentence by accessing the relevant information from the external knowl- edge base. Our novel contribution is unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three do- mains: COVID-19, News, and Conversa- tions, and achieve signiﬁcant performance improvements compared to the original RAG model. Our work has been open-sourced through the Huggingface Transformers li- brary, attesting to our work’s credibility and technical consistency. 1\\n\\ntask in natural language understanding. ODQA methods generally feature a two-stage pipeline: a retriever that selects passages relevant to a given question and a reader that generates the answers from selected passages. Conventionally, these two components are trained separately using ground truth context passages relevant to question-answer (QA) pairs. However, for many real-world scenar- ios, it is hard to ﬁnd explicitly annotated context- question-answer triplets (Lee et al., 2019; Lewis et al., 2020b; Guu et al., 2020).\\n\\nRecently, Retrieval Augmented Models (RAGs) have drawn considerable attention from researchers. RAG consists of a state-of-the-art-neural retriever called Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and BART seq2seq language model (Lewis et al., 2020a). Compared to the conventional two-staged ODQA pipelines, RAG merges the retriever and reader stages into one architecture. Moreover, unlike expensive lan- guage models with billions of parameters (e.g., GPT-3 (Brown et al., 2020) and Megatrone- LM (Narayanan et al., 2021)) where the model’s parametric memory represents the complete knowl- edge, RAG can also extract knowledge from an ex- ternal knowledge base. Using both parametric and non-parametric memory generally leads to reduced hallucinations and higher interpretability in tasks like question answering and summarization (Xu et al., 2021; Komeili et al., 2021; Guu et al., 2020; Lewis et al., 2020b).\\n\\n1\\n\\nIntroduction\\n\\nOpen Domain Question Answering (ODQA) (Lee et al., 2019; Lewis et al., 2020c) is an important\\n\\n1This paper is awaiting publication at TACL and this is a\\n\\npre-MIT Press publication version\\n\\nIn this work, we focus on exploring retrieval augmented architectures for the task of domain- speciﬁc open-domain question answering. Al- though there are several similar retrieval augmented architectures, such as REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2021), we used Re-'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='trieval Augmented Generation (RAG) in our exper- iments due to its excellent open-source documenta- tion and availability.\\n\\nWhen the RAG model is ﬁnetuned for down- stream QA tasks, the original implementation keeps the encoding of passages and the external knowl- edge base ﬁxed. This is because re-encoding the external knowledge base is computationally ex- pensive and relies on a sophisticated implementa- tion. Despite not ﬁnetuning the passage encodings, the RAG model performs well for datasets with Wikipedia-like knowledge bases because the DPR retriever components have already been trained on Wikipedia-based datasets (Kwiatkowski et al., 2019; Joshi et al., 2017). However, the feasibility of adapting RAG to speciﬁc ODQA domains such as research papers and news is not well understood. This is a critical research gap to address, as im- proved domain adaptation can further improve the ODQA performance of RAG.\\n\\nThis paper explores the feasibility of using RAG in specialized domains for ODQA. In particular, we propose two modiﬁcations to the original RAG to improve its domain adaptability. Motivated by recent end2end retrieval augmented mecha- nisms (Guu et al., 2020; Sachan et al., 2021; Singh et al., 2021), we ﬁrst propose a method to ﬁne- tune the RAG model with its neural retriever and update its knowledge encodings asynchronously during training. We refer to this as RAG-end2end since it allows us to update all RAG components during training, including the external knowledge base, the DPR model, and the BART model. Sec- ondly, we propose an auxiliary training signal to help our model learn more domain-speciﬁc knowl- edge. This took the form of generating a concise and factual statement about a document using a self-retrieved set of passages from the provided domain-speciﬁc knowledge base. These two mod- iﬁcations offer a unique feature to RAG-end2end over RAG: joint training of the retriever and gen- erator for the end QA task and domain adaptation. Although asynchronous updates to the knowledge encoder have been proposed before in the REALM, previous work has not evaluated the effects of joint training of the RAG’s retriever and the generator for the domain adaptation in ODQA.\\n\\nWe evaluate our proposed approach on three dif- ferent datasets from three domains: COVID-19 research (Wang et al., 2020), Conversations (Wu et al., 2021b), and News (Trischler et al., 2016).\\n\\nThe major ﬁnding of our work is that the adap- tation of the retriever component plays a critical role in overall domain adaptation performance in RAG-like architectures. Updating only the ques- tion encoder without updating the knowledge base encoding could degrade performance. Instead of ﬁnetuning the DPR retriever separately, our ex- periments show that ﬁnetuning it as a part of the RAG-end2end mechanism gives better overall re- sults. Our results also show that using the auxiliary signal improves both the retriever component and the overall accuracy.\\n\\nIn addition, we open-source the implementation of RAG-end2end with the HuggingFace Transform- ers (Wolf et al., 2019) Library2 providing the oppor- tunity for the scientiﬁc community to use/test/build on our work.\\n\\n2 Background and Related Work'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='2 Background and Related Work\\n\\nOpen-domain QA systems (Yang et al., 2015; Kwiatkowski et al., 2019) generally have a two- stage pipeline: passage retrieval (i.e., ﬁnding rele- vant text chunks related to an input question from a knowledge base) and machine comprehension (i.e., generating an answer from a set of selected documents). Traditionally sparse vector methods such as TF-IDF and BM25 are used for document retrieval (Robertson and Zaragoza, 2009). Re- searchers have recently moved to use dense text representations, which allows modeling textual sim- ilarity more semantic level. A recent example is the ‘Dense Passage Retriever (DPR)’ (Karpukhin et al., 2020), which generates embeddings for questions and text passages using two BERT (Devlin et al., 2018) models. The dot product of the embeddings is used as a similarity score between a question and a passage. DPR has demonstrated that higher retrieval precision results in a higher end-to-end QA accuracy. For the answer generation compo- nent of QA systems, recent studies have used either extractive language models like BERT or genera- tive language models like BART/GPT-2 (Min et al., 2021; Lewis et al., 2021).\\n\\n2.1 Retrieval Augmented Architecture\\n\\nRecently, Retrieval Augmented Architectures (Lewis et al., 2020b; Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain\\n\\n2Huggingface Transformers implementation\\n\\nQA architectures, RAG (Lewis et al., 2020b) com- bines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia arti- cles indexed with the FAISS library (Johnson et al., 2017). RAG ﬁrst encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can ﬁnetune both the generator and the question en- coder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG’s ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019). Other recent work also highlights how the outputs generated from RAG models are much more factual due to RAG being conditioned on the retrieved documents, possibly providing an an- swer to the hallucination problem of generative language models. Shuster, Kurt, et al. (Shuster et al., 2021) also highlight how RAG reduces hal- lucinations in knowledge-grounded conversational tasks, where the task is to generate responses to dialogues based on a large Wikipedia knowledge base. Xu et al. (2021) illustrate the effectiveness of RAG in chat-bot frameworks and highlight how RAG models are able to recall and summarize con- versations compared to standard seq2seq models with only parametric memory. This paper aims to understand how RAG could be extended to an end2end model and adapted to speciﬁc domains. To the best of our knowledge, this is the ﬁrst time RAG is being investigated on domain adaptation for the task of ODQA systems.\\n\\n2.2 REALM-like end2end Retrieval Augment\\n\\nArchitectures\\n\\nREALM (Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that in- In the volves an end-to-end trainable retriever. REALM work, the authors ﬁrst train the entire model on the masked language prediction task and then ﬁne-tune it on question-answering tasks (keeping the retriever frozen). In comparison to REALM, the original RAG model uses an already trained DPR retriever and conducts partial end-to-'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='end training with a BART reader model. Com- pared to REALM, RAG is less computationally expensive, and its code is available open-source. We explore and extend the original RAG archi- tecture for domain adaptation in our work. We adapted some concepts of our RAG-end2end ex- tension from REALM. REALM only updates its retriever during the pre-training process that uses the masked language modeling (MLM) (Devlin et al., 2018) task. Then during the downstream ﬁne-tuning task, REALM keeps its retriever ﬁxed. However, the REALM end-to-end training code is not open-sourced, possibly due to its computa- tional complexity. Compared to REALM, RAG is a combination of already pre-trained language models where the users do not need to go through a heavy pre-training stage. Due to these engineering- friendly features and high availability, we con- ducted our experiments with RAG and extended RAG into an end-to-end trainable retrieval augmen- tation model. It is also important to highlight that none of the prior work has explored the domain adaptation of retrieval augment models for question answering; instead, most focus on general question answering with Wikipedia-based knowledge bases. Similar to REALM’s end2end architecture, re- cent work (Sachan et al., 2021) extended RAG and highlighted that the retriever training could improve the overall performance in question- answering datasets like Natural Questions. Com- pared to our work, the authors did not focus on the domain adaptation of retrieval augment mod- els. The authors mainly explore the ability to train neural retrievers in an end-to-end way using re- trieval augment models. Similarly, another related work (Singh et al., 2021) extended retrieval aug- mented architectures to an end-to-end model and illustrated that it could improve the question an- swering accuracy. Singh et al. (2021) mainly fo- cused on improving the document reading ability and answer generation rather than domain adapta- tion.\\n\\n3 Model Architecture and Training\\n\\nProcedure\\n\\nIn this work, we extend RAG to ﬁnetune all compo- nents, including the DPR retriever, and dynamically update the external knowledge base during train- ing. We hypothesize that the use of asynchronous updates helps with domain adaptation. Figure 1 demonstrates the main workﬂow of our model. In\\n\\nJointLoss\\n\\nStatementsCough (82.5%), fever(75%) were the most common symptom\\n\\nFAISSSimilarity Retriever\\n\\nIndexed Knowledge Base\\n\\nExternal KnowledgeBase\\n\\nBART\\n\\nEncoded KnowledgeBase\\n\\nGeneratedAnswers“Fever”\\n\\nFAISSIndexer\\n\\n1. ”Other symptoms observed weremyalgia and headache\"2. ”66% cases hadcrepitations and 42% had wheezing\"3. ”common symptoms on admission included fever and cough”… GPU 2GPU 1SHARD 2SHARD 1\\n\\nQuestionsWhat is the most common symptom of covid-19?\\n\\nDPR InputEncoder\\n\\nAsynchronous re-encoding Asynchronous re-indexing\\n\\nDPR PassageEncoder\\n\\nGeneratedParaphrase“Fever is the most common symptom”Main training process\\n\\nRetrieved Passages\\n\\nFigure 1: System Overview. Our RAG-end2end training architecture uses asynchronous processes to dynamically re-encode and re-index the knowledge base while optimizing a joint QA and paraphrasing signal loss. The training dataset consists of both reconstruction signals and QA pairs. The network learns to generate answers to questions and useful statements jointly. The input to the BART reader is illustrated in Equation 3, where the model can differentiate the answer generation task and statement reconstruction task with the use of a control token. During the training, embeddings and the knowledge base index get updated asynchronously.\\n\\nthe following sections, we describe our extensions and training signals.\\n\\n3.1 RAG Retriever and Generator\\n\\nPRAG−Token−Loss(y|x) =\\n\\nn ∏ i\\n\\n∑ zεtop−kP(.|x)\\n\\nPη (z|x)Pθ (yi|x, z, y1:i−1)'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='the following sections, we describe our extensions and training signals.\\n\\n3.1 RAG Retriever and Generator\\n\\nPRAG−Token−Loss(y|x) =\\n\\nn ∏ i\\n\\n∑ zεtop−kP(.|x)\\n\\nPη (z|x)Pθ (yi|x, z, y1:i−1)\\n\\nThe retriever is a DPR (Karpukhin et al., 2020) model pre-trained on Wikipedia-based question- answering datasets (Kwiatkowski et al., 2019; Joshi et al., 2017). It consists of two tower BERT-based networks: the Question Encoder (EQ) and the Pas- sage Encoder (EP). We use their CLS token embed- dings as representations for questions and passages. The similarity between a question (q) and a passage (p) is calculated by taking the dot product of the two embeddings as shown in Equation 1.\\n\\nsim(p, q) ∝ EQ(q)T EP(p). (1) RAG’s generator consists of a pre-trained BART (Lewis et al., 2019) seq2seq language model. To train these retriever and generator com- ponents, RAG enhances the traditional sequence- to-sequence cross-entropy loss function by setting the retrieved passages as a latent variable (Z) (Guu et al., 2020; Lewis et al., 2020b). The loss value of generating each token is marginalized on the probability of selecting documents given a context X (i.e., Document Score p(Z|X)). The formula (RAG-Token-Loss) can be written as illustrated in Equation 2.\\n\\n3.2\\n\\nIndexing of the External Knowledge Base\\n\\nBefore the training phase, we need to encode all passages in the external knowledge base using EP. Then we need to retrieve similar passages from the external knowledge base given the output from EQ. This process mainly involves dot product cal- culation between input question embeddings and encoded passages. The retrieval process will likely result in a performance bottleneck during the train- ing since there are usually millions of passages in the knowledge base. To address this issue, RAG adopts the FAISS indexing approach proposed in (Johnson et al., 2017). With the help of the indexes, we can skip a considerable amount of repeated com- putation and signiﬁcantly accelerate the retrieval process.\\n\\n3.3 End-to-End Retriever Training\\n\\nAlthough the DPR module makes use of two BERT models (EP,Eq), the original RAG architecture only ﬁne-tunes the question encoder EQ in the retriever. The passage encoder EP and the external knowl- edge base’s encoding are ﬁxed during the training\\n\\n(2)\\n\\nphase. In other words, the pre-trained passage en- coder of DPR is only used once to encode the ex- ternal knowledge base. The RAG authors suggest that such a design performs well for Wikipedia- based ODQA datasets (Kwiatkowski et al., 2019; Joshi et al., 2017). Such settings work because the DPR model was also pre-trained with Wikipedia- based datasets, and their experiment uses an exter- nal knowledge base consisting of Wikipedia arti- cles.\\n\\nHowever, it may be beneﬁcial to ﬁne-tune all the DPR components during RAG training for domain adaptation since the model needs access to differ- ent domain-speciﬁc external knowledge bases. In this work, we introduce RAG-end2end, where we augment RAG to be fully end-to-end trainable. We ﬁne-tune the passage encoder and question encoder and then update the index of the external knowl- edge base during the training process.'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='It is straightforward to propagate gradients to both the passage and question encoders with RAG’s loss function. Because this loss function employs the passage selection probability known as doc- score(pη (z|x) term illustrated in Equation 2). How- ever, for it to have a true effect on the overall model training process, we have to iteratively update the embeddings with the updated context encoder and then update the index of the external knowledge base. In other words, we need to re-encode and re-index the knowledge base using the updated passage encoder. When the external knowledge base possesses tens of millions of passages, the re- encoding and re-indexing steps can be very time- consuming. Re-encoding can take several GPU hours, and re-indexing with FAISS can take several CPU hours, depending on the size of the knowledge base. Therefore, it is inefﬁcient to stall the training loop while the re-encoding re-indexing steps are being carried out.\\n\\nTo have an efﬁcient training mechanism, we de- signed our training framework into three main pro- cesses: (1) The main training loop, which updates the gradients, (2) Re-encoding processes with sev- eral GPUs that update the knowledge-base encod- ing with the updated DPR’s context encoder, and (3) A Re-indexing process that uses FAISS to build an index with the updated encoding. Figure 1 illus- trates these three processes. Our implementation uses two asynchronous processes to re-encode and re-index the external knowledge base that runs in- dependently to the main training loop. We ﬁrst\\n\\ndistribute the external knowledge base to a set of GPUs that are not used in the main training loop. Then we encode the passages with an updated pas- sage encoder which we call the re-encoding pro- cess. Once the re-encoding process has ﬁnished, we re-index the knowledge base in another paral- lel process that uses FAISS (re-indexing process). Inside the main training loop, we ensure that the re-indexing process always starts after ﬁnishing the re-encoding process. Then as soon as the new index of the external knowledge base is created, we load that to the main training loop. Once the new index loading is completed again, we start the re-encoding process, which repeats the entire em- bedding updating process. It is important to note that the ﬁrst re-encoding process should get ﬁn- ished, and new embeddings should get saved to the hard disk before the start of the FAISS indexing pro- cess. If the knowledge base is not entirely updated with the new embeddings, the re-indexing process fails. We use python multiprocessing handles to keep the order, and re-indexing and re-encoding processes are only asynchronous with respect to the main training loop process. The number of steps between each re-encoding process depends on the size of the dataset. To test the number of steps between the knowledge-base updates, we ex- perimented with a knowledge base consisting of 250,000 passages and used four dedicated GPUs for the re-encoding process with a batch size of 32 each. Our computation machine consists of 96 CPU cores. We found that it takes an average of 750 updates. However, the computation time can be easily improved when using more GPUs for en- coding and using a machine with a higher number of CPU cores (FAISS indexing process depends on the number of CPU cores). These steps are repeated throughout the training loop. Since the training and knowledge base’s index update pro- cesses are running asynchronously, it may result in stale gradients. This, however, does not signiﬁ- cantly degrade the model performance according to previous research (Guu et al., 2020).\\n\\n3.4 Statement Reconstruction\\n\\nWe explore the incorporation of statement recon- struction as an auxiliary signal assuming that it forces the model to gain more domain-speciﬁc knowledge. As illustrated in Figure 1, we ﬁrst encode input statements using the input/question encoder (EQ). Then the retriever retrieves the most'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='similar set of passages from the indexed external knowledge base by conducting a similarity search. Afterward, the ﬁnal output generator attempts to re-construct the input statements using only the se- lected support set of documents. We ensure that the external knowledge base does not contain the input statement to prevent the model from overﬁt- ting on just the lexical content. To differentiate the paraphrasing signal from the QA signal, we prepend a special token < p > (represents pas- sages) in front of the reconstruction statements, which acts as a control token in the seq2seq lan- guage modeling (Raffel et al., 2019; Keskar et al., 2019). Concretely, when training the RAG archi- tecture on QA pairs, the questions are prepended to the retrieved passages before being fed to the BART generator. As illustrated in Equation 3, for the input reconstruction signal, we only prepend the < p > token to the retrieved passages before feeding them to the BART generator.\\n\\nQA INPUT: RECONSTRUCTION INPUT:\\n\\n(cid:104)Question(cid:105) + (cid:104)Retrieved Passages(cid:105) (cid:104)<p>(cid:105) + (cid:104)Retrieved Passages(cid:105)\\n\\n4 Experiments & Results\\n\\n4.1 Domain Speciﬁc Dataset Setup\\n\\nIn this work, our main intention is to explore the adaptation of domain-speciﬁc retrieval augmenta- tion with regard to ODQA. As mentioned in the recent work (Lewis et al., 2020b), most ODQA datasets like Natural Questions (Kwiatkowski et al., 2019) , TriviaQA (Joshi et al., 2017), WebQues- tions (Berant et al., 2013), and CuratedTrec (Baudiš and Šediv`y, 2015) are answered with Wikipedia- based knowledge-bases. Since neural retrievers like DPR are already trained with Wikipedia-based datasets, it is hard for us to explore the domain adaptation of RAG fairly in this setting. There- fore, we selected three domain-speciﬁc datasets for our experiment: COVID-19 QA, News QA, and Conversation QA. Since the availability of domain- speciﬁc ODQA datasets is minimal, in our work, we open-source all domain-speciﬁc knowledge- bases and question-answer pairs to support future research3. COVID-19 QA Domain Knowledge Base Generation: To create the external knowledge base, we use 5,000 full-text scientiﬁc articles extracted from the CORD-19 (Wang et al., 2020) dataset. The external knowledge base is created with 250,000 100-word passages. Each\\n\\n3domain speciﬁc datasets\\n\\n(3)'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='3domain speciﬁc datasets\\n\\n(3)\\n\\npassage is pre-pended with the title of the research paper. Reconstruction Statement Generation: We use sen- tences from the abstract section of research articles for the reconstruction signal. We ﬁrst extract the abstract sections in 10K papers and split them into sentences using the NLTK library (Loper and Bird, 2002). We ﬁlter out the sentences that are too short (less than 15 words) or too long (more than 35 words). In this process, approximately 50,000 ab- stract statements are generated. It is important to note that when generating the knowledge base, we exclude the abstract sections. Synthetic QA Generation: In this domain, we only use synthetic data for training and validation. Fol- lowing the prior work (Shakeri et al., 2020), we use a BART seq2seq model trained on the SQuAD dataset (Rajpurkar et al., 2016) to generate syn- thetic QA pairs given a passage. We used the Squad dataset’s passages as the input and corresponding question-answer pairs as the expected output. We trained a BART-large checkpoint for two epochs. Then, we followed round-trip consistency (Alberti et al., 2019) to ﬁlter synthetic QA pairs. Our ﬁnal synthesized QA dataset consisted of 225,000 QA pairs. We use 90% of these QA pairs as training data and 10% as validation data. As the test data, we use 2000 human-labeled question-answer pairs from the COVID-QA dataset (Moller et al., 2020). News QA Domain Knowledge Base Generation: We extract 85,000 100-word passages as the knowledge base us- ing 10,000 news articles from the NewsQA dataset (Trischler et al., 2016). Reconstruction Statement Generation: We extract corresponding news summary sentences from the CNN/DM dataset (Hermann et al., 2015) for the re- construction signal. Every article consists of more than one summary sentence. However, we use the ﬁrst sentence as the title of the article, which we used in knowledge base generation and the rest of the statements as reconstruction statements. Our ﬁnal dataset contains 35,000 summary statements. QA Generation: The NewsQA dataset (Trischler et al., 2016) consists of 100,000 human anno- tated QA pairs from 10,000 news articles from the CNN/DM dataset (Hermann et al., 2015). We use the train (90,000), valid (5,000) and test (5,000) splits given in the dataset to train and evaluate our model. All questions in the NewsQA dataset focus on the high-level content of articles. So, to answer\\n\\nthese questions, the model must access a large span of passages to conduct the reasoning process. Conversation QA Domain Knowledge Base Generation: We create the exter- nal knowledge base of 110,000 passages by split- ting the 10,000 conversations given in the QAConv dataset (Wu et al., 2021b) into passages, each with at most 100 words. We prepend the identiﬁer of each conversation (found in the original dataset) as the title of the passages. We also appended the speaker’s name, followed by the \":\" symbol, to the starting position of each dialogue to keep each conversation connected to its speakers. Reconstruction Statement Generation: We use the state-of-the-art abstractive conversation summa- rization model4 (Wu et al., 2021a) to generate one-sentence (TLDR) summary (approximately 45 words per conversation). We then use this as the auxiliary signal. We only generate sum- maries of conversations with more than 45 words. By doing this, we collect 35,000 synthetic sum- mary/reconstruction statements. QA Generation: We use the QAConv dataset (Wu et al., 2021b), which contains 35,000 QA pairs generated from 10,000 conversations that involved two or more parties. We use the train (25,000), valid (5,000) and test (5,000) splits given in the dataset to train and evaluate our model.\\n\\n4.2 Training and Evaluation Setup'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='4.2 Training and Evaluation Setup\\n\\nWe use the HuggingFace-Transformers (Wolf et al., 2019) library to implement the RAG-end2end ar- chitecture. We initialize the DPR and BART mod- els of using the open-source HuggingFace check- points5. Prior to ﬁne-tuning, we index and encode the external knowledge base using FAISS. We se- lect HNSW FLAT as the indexing mechanism (with 128 bi-directional links). We use 100 words as the maximum passage length as suggested by the prior RAG work (Lewis et al., 2020a). During train- ing, we use six Tesla V100 GPUs with 32 GBs of memory. Four of them are used for training, and two are used for re-encoding. We train each RAG model variant 4.2 for ten epochs and select the ﬁnal checkpoint with the highest validation accuracy.\\n\\nWe use the Exact Match (EM), F1 score, and Top-K retrieval accuracy as evaluation metrics. The EM score computes the word level exact match be- tween the predicted answer and the real answer.\\n\\n4salseforce checkpoint 5rag-token-base checkpoint\\n\\nThe F1-score calculates the number of words in the predicted answer that are aligned with the real an- swer regardless of the order. The Top-k retrieval ac- curacy is calculated by matching the answer strings with the contents of the retrieved k passages.\\n\\nWe compare RAG and RAG-end2end in the fol-\\n\\nlowing ﬁve scenarios.\\n\\n1. RAG-original. This model is ﬁnetuned on the natural question dataset (Kwiatkowski et al., 2019) with the Wikipedia knowledge base and serves as the non-domain adapted baseline 6. This model is not ﬁnetuned with domain-speciﬁc question-answer pairs, and we report the zero-shot performance.\\n\\n2. RAG-original-QA. This is the original RAG model ﬁnetuned with only domain-speciﬁc question-answer pairs.\\n\\n3. RAG-end2end-QA. This is the RAG model with our end2end retriever extensions and ﬁnetuned only with domain-speciﬁc question- answer pairs.\\n\\n4. RAG-original-QA + R. This is the RAG original model ﬁnetuned with both domain- speciﬁc question-answer pairs and our recon- struction signal.\\n\\n5. RAG-end2end-QA + R. This is the RAG model with our end2end retriever extensions and trained with both question-answer pairs and our reconstruction signal.\\n\\nWe present the results of each scenario in Table 1.\\n\\n4.3 Effect of End-to-End Retriever Training\\n\\non Domain Adaptation\\n\\nWe ﬁrst test if ﬁnetuning of both the passage en- coder and question encoder of the RAG’s retriever while updating the external knowledge base would improve domain adaptation. We compare the per- formance of RAG-original-QA and RAG-end2end- QA, isolating any performance improvement due to the reconstruction signal. The results in Table 1 illustrate that RAG-end2end-QA signiﬁcantly out- performs RAG-original-QA on all metrics – EM, F1, Top-5, and Top-20 – across all three domains. The improvements in the EM score varied from 1.13 points in the News domain to 12.16 points in the Conversation domain.\\n\\n6public rag-token-nq checkpoint\\n\\nModel Name\\n\\nEM\\n\\nF1\\n\\nTop-5 Top-20\\n\\nCOVID-19 Domain\\n\\n(1) RAG-original\\n\\n0.0\\n\\n4.73\\n\\n10.56\\n\\n15.69\\n\\n(2) RAG-original-QA\\n\\n2.95\\n\\n12.01\\n\\n12.29\\n\\n18.43\\n\\n(3) RAG-end2end-QA\\n\\n8.08\\n\\n18.38\\n\\n19.85\\n\\n26.91\\n\\n(4) RAG-original-QA+R\\n\\n3.66\\n\\n12.20\\n\\n12.79\\n\\n18.45\\n\\n(5) RAG-end2end-QA+R 8.32\\n\\n19.57\\n\\n23.05\\n\\n31.23\\n\\nNews Domain\\n\\n(1) RAG-original\\n\\n4.33\\n\\n7.92\\n\\n19.46\\n\\n30.33\\n\\n(2) RAG-original-QA\\n\\n7.26\\n\\n14.26\\n\\n22.86\\n\\n34.55\\n\\n(3) RAG-end2end-QA\\n\\n8.39\\n\\n16.31\\n\\n28.89\\n\\n41.20\\n\\n(4) RAG-original-QA+R\\n\\n8.62\\n\\n16.46\\n\\n27.28\\n\\n39.56\\n\\n(5) RAG-end2end-QA+R 14.08\\n\\n23.7\\n\\n39.67\\n\\n50.95\\n\\nConversation Domain\\n\\n(1) RAG-original\\n\\n5.49\\n\\n9.27\\n\\n12.14\\n\\n20.02\\n\\n(2) RAG-original-QA\\n\\n12.09\\n\\n20.05\\n\\n22.73\\n\\n32.05\\n\\n(3) RAG-end2end-QA\\n\\n24.25\\n\\n36.05\\n\\n46.01\\n\\n55.55\\n\\n(4) RAG-original-QA+R\\n\\n14.21\\n\\n24.62\\n\\n26.32\\n\\n36.21\\n\\n(5) RAG-end2end-QA+R 25.95\\n\\n37.96\\n\\n49.11\\n\\n58.75\\n\\nTable 1: Domain adaptation Performance of differ- ent RAG models used in our experiments. We illus- trate the results related to all three domains. Details about each model are described in Section 4.2'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='Evaluating the performance of passage retrieval using Top-5 and Top-20 scores, we see a marked increase of around 25 points in the conversation domain, with the other domains showing improve- ments of between 4.7 to 6.6 points.\\n\\nAbove all, these results suggest that ﬁne-tuning both the passage and question encoders of the RAG’s retriever while updating the external knowl- edge base can improve domain adaptation.\\n\\n4.4 Effect of Adding the\\n\\nStatement-Reconstruction Auxiliary Task\\n\\nIn this experiment, we test our next hypothesis: adding the auxiliary training signal of statement reconstruction along with QA pairs improves do- main adaptation. We compare the performance of RAG-end2end with and without the reconstruc- tion signal by comparing the performance of RAG- end2end-QA + R and RAG-end2end-QA in Ta- ble 1. This shows that RAG-end2end-QA + R outperforms RAG-end2end-QA for all three do- mains. The range of increases in the EM scores varied from 1.7 points in the conversation domain to an 8.39 points increase in the News domain. The top-20 retrieval accuracy also increased in a range between 3.2 to 8 points.\\n\\nWe further compare the effect of adding the re-\\n\\nconstruction signal to RAG-original by comparing RAG-Original-QA with RAG-Original-QA + R. We ﬁnd that even without the end2end extension, the reconstruction signal improves the performance moderately. This improvement in the EM score ranged from 0.84 points in the COVID-19 domain and 3.12 points in the Conversation domain.\\n\\nFinally, we highlight the overall improvement of our contributions by comparing RAG-Original- QA with RAG-end2end-QA+ R. As the most sig- niﬁcant improvement; we highlight the 13-point improvement of EM score for the Conversation do- main. For retrieval performance, we highlight the 27-point improvement in the top 5 and 16 improve- ments in the top 20 for the Conversation domain.\\n\\nTo demonstrate the reconstruction statement gen- eration, we provide an example of the generated reconstruction output of given a statement for each domain using the RAG-end2end-QAR + R model in Table 2. The second column contains the in- put statements with the special token < p >, the third column shows a snapshot of retrieved top- 5 documents, and the ﬁnal column shows the re- constructed statements. As the reconstruction state- ments demonstrate, we highlight that the model can generate statements close enough to the input.\\n\\n4.5 Retriever’s domain adaptation with\\n\\nRAG-end2end\\n\\nAn important part of our RAG-end2end extension is updating the entire DPR retriever during training. Previous work (Ma et al., 2020) has explored the importance of the domain adaptation of neural retrievers and highlighted the performance gains in domain-speciﬁc retrieval tasks. We argue, based on our above-mentioned RAG end2end’s retriever performances and prior work, that when adapting RAG to various domains, having a domain-speciﬁc retriever plays a key role in achiev- ing good performance. However, this end-to-end RAG ﬁnetuning can get computationally costly, especially with the number of passages in the external knowledge base where they should get re-encoded and re-indexed. Instead of ﬁnetuning DPR as a part of RAG-end2end, an alternative approach is to ﬁnetune DPR on domain-speciﬁc data separately on its vector similarity-based loss function (Karpukhin et al., 2020) and then initializing the RAG architecture prior to ﬁnetuning with the QA data. We explore if RAG-end2end can perform on par if we initialize a RAG model\\n\\nDomain\\n\\nInput Statement\\n\\nMost Similar Retrieved Document\\n\\nReconstructed Statement\\n\\nCOVID-19\\n\\n<p>Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs.'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='Reconstructed Statement\\n\\nCOVID-19\\n\\n<p>Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs.\\n\\nMost common signs and symptoms on ad- mission included fever and cough . Of all children, 32% had complaint of difﬁculty in respiration. Other symptoms observed were myalgia, headache and vomiting . On examination, 66% cases had crepitations and 42% had wheezing. Hypoxemia was observed in 31% cases at admission\\n\\nThe most common signs and symptoms on admis- sion were fever and cough, and 32% had complaint of difﬁculty breathing\\n\\nNews\\n\\n<p>Capsule was carrying South Korea’s ﬁrst astro- naut .\\n\\nMOSCOW, Russia Russian space ofﬁcials say the crew of the Soyuz space ship is resting after a rough ride back to Earth. A South Korean bioengineer was one of three people on board the Soyuz capsule. The craft carrying South Korea’s ﬁrst as- tronaut landed in northern Kazakhstan on Saturday, 260 miles off its mark, they said.\\n\\nSoyuz capsule carrying South Korea’s ﬁrst astro- naut lands 260 miles off its mark.\\n\\nConversation <p>The Supreme Court will hear the case on the grounds of First Amend- ment protection of free speech.\\n\\n(PETITIONER): Yes, Your Honor. Mr. Tory, who was appearing pro se in the trial court, from the very outset objected that he was being held liable for speech protected by the First Amendment. <end>$\\n\\nJustice Souter and MR. CHEMERINSKY are ar- guing that the injunction in the case of Tory should not be applied\\n\\nTable 2: Examples of Reconstructed Statements. Reconstructions generally capture the context of the retrieved documents and are similar to the input statement but are not always factually 100% correct (e.g. COVID-19 example). Input statement column shows the input to the model with the special <p> token. The Retrieved Documents shows a snap-shot of the top-retrieved document used to re-construct the statement\\n\\nwith an independent domain-adopted DPR model. This helps us further understand the ability of the RAG-end2end extension to ﬁnetune the retriever with domain-speciﬁc data.\\n\\nStandalone DPR ﬁne tuning with domain speciﬁc data\\n\\nThe standalone DPR can be ﬁnetuned if we have access to gold-standard passages that contain the answers for given questions and hard negative pas- sages which consist of similar details to the ques- tion but not the exact answers. DPR uses a dot- product-based similarity loss, capturing the simi- larity between the correct passage for the question while comparing with some hard-negative exam- ples (Karpukhin et al., 2020) (which are lexically similar but do not contain the answer) (Karpukhin\\n\\net al., 2020). We use the deep-haystack framework7 to ﬁnetune DPR for each domain using domain- speciﬁc data. We created ﬁnetuning datasets for all three domains. First, for the Covid-19 do- main, we utilized the synthetic question-answer pairs and their relevant passages that consist of 100 words. The use of domain-speciﬁc synthetic QA pairs for DPR ﬁnetuning has already shown permanence improvements (Ma et al., 2020). For hard-negative examples, we used BM-25 lexical matching search as mentioned by the DPR authors, where we retrieved passages that do not contain the answer based on their lexical similarity with the question. Although for the News domain and the Conversation domain, we have a supervised dataset where we can map questions into the cor- rect passage, we did not get better results after ﬁnetuning the original DPR using the supervised\\n\\n7deepset-ai\\n\\nModel Name\\n\\nTop-5 Top-20\\n\\nCOVID-19 Domain\\n\\n(1) DPR-original\\n\\n9.39\\n\\n14.72\\n\\n(2) DPR-domain-adapted\\n\\n13.66\\n\\n20.01\\n\\n(3) DPR-RAG-end2end\\n\\n20.64\\n\\n28.21\\n\\nNews Domain\\n\\n(1) DPR-original\\n\\n20.95\\n\\n31.04\\n\\n(2) DPR-domain-adapted\\n\\n20.98\\n\\n31.92\\n\\n(3) DPR-RAG-end2end\\n\\n39.67\\n\\n50.95\\n\\nCoversation Domain\\n\\n(1) DPR-original\\n\\n15.15\\n\\n23.95\\n\\n(2) DPR-domain-adapted\\n\\n23.15\\n\\n34.53\\n\\n(3) DPR-RAG-end2end\\n\\n49.11\\n\\n58.75'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='(2) DPR-domain-adapted\\n\\n20.98\\n\\n31.92\\n\\n(3) DPR-RAG-end2end\\n\\n39.67\\n\\n50.95\\n\\nCoversation Domain\\n\\n(1) DPR-original\\n\\n15.15\\n\\n23.95\\n\\n(2) DPR-domain-adapted\\n\\n23.15\\n\\n34.53\\n\\n(3) DPR-RAG-end2end\\n\\n49.11\\n\\n58.75\\n\\nTable 3: Comparison of DPR models ﬁnetunned on domain speciﬁc data against publicly available DPR checkpoint which is trained on Wikipedia domain for all three domains.\\n\\ndata. The main reason for the degradation of perfor- mance is the length of the correct passage related to the question. In both News and Conversation domains, most of the questions come from longer passages, whereas the pre-trained DPR only ac- cepts 100-word passages. To mitigate this issue, we generated synthetic question-answer pairs with the external knowledge bases of news and Conver- sation domains similar to the COVID-19 domains by following the same procedure mentioned in Sec- tion 4.1. Then the hard-negative examples were also mined according to the above-mentioned BM- 25 lexical matching method. After training, we evaluate the DPR retrieval accuracy using the test dataset and external knowledge base for each do- main, similar to the RAG’s retrieval evaluation we conducted in Section 4.2\\n\\nTable 3 compares (1) DPR-orignal, which is the publicly available checkpoint8 trained on Wikipedia, with (2) DPR-domain-adapted, which is the ﬁnetuned model with DPR’s original loss function. The (3) DPR-RAG-end2end is the re- trieval part of RAG-end2end-QA + R from Ta- ble 1 for comparison. We include the DPR-RAG- end2end model to highlight the improvement of the DPR model as a result of RAG-end2end training with both training signals. When comparing the DPR-RAG-end2end model with the other variants in Table 1, we observe that the RAG-end2end archi- tecture signiﬁcantly improves the DPR’s domain\\n\\n8DPR-checkpoint\\n\\nadaptation for all three domains. Therefore, in fu- ture work, RAG-end2end could be used as a way to train a neural retriever, which could beneﬁt even for retrieval-only applications.\\n\\nAs shown in Table 3, we observe that ﬁne-tuning DPR models on the original DPR loss function using domain-speciﬁc data improves the overall retrieval performance for each domain. For the Covid-19 and Conversation domains, there’s a clear improvement in the top-5 and top-20 retrieval accu- racies. We observed almost the same results for the News domain compared to the original DPR. This could be due to similar kinds of data in Wikipedia, which were originally used to train the DPR and CNN/DM text.\\n\\nOverall as illustrated in Table 3, we highlight the fact that the RAG-end2end’s loss function has the ability to adapt the DPR to speciﬁc domains better than ﬁne-tuning the DPR with the passages and question pairs for each domain. The improve- ments for all three domains in top-5 and top-20 retrieval accuracies of DPR-RAG-end2end com- pared to DPR-original and DPR-domain-adapted is noticeable. These results further highlight the ability of RAG-end2end to ﬁne-tune or improve its retriever.\\n\\nInitializing RAG with domain adapted DPR prior to ﬁnetuning Next, we investigate the performance of RAG models when initialized with a domain-adapted DPR. We initialize RAG’s question encoder and the passage encoder with DPR-domain-adapted (from trained models illustrated in Table 3) and ﬁnetune RAG with the settings of RAG-original-QA+R. The objective is to compare how the RAG mod- els initialized with domain adopted DPR models perform in comparison to using the RAG-end2end extension.\\n\\nTable 4 demonstrates results from four models. (1) RAG-original-QA+R and (3) RAG-end2end- QA+R are taken from the main results (Table 1). The (2) RAG-original-QA+R (DPR-adapted) model was ﬁrst initialized with a domain-adopted DPR model (from Table 3) before being ﬁnetuned with domain-speciﬁc QA pairs and re-construction signals with the RAG-original settings.'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='The results in Table 4 indicate that for all do- mains, ﬁnetuning the RAG-original with a domain- adapted DPR gives higher performance than ﬁne- tuning the RAG-original with the usual DPR model checkpoint (Compare (1) and (2) in the Table 4).\\n\\nModel Name\\n\\nEM score\\n\\nF1 Score Top-5 Top-20\\n\\nCOVID-19 Domain\\n\\n(1) RAG-original-QA+R\\n\\n3.66\\n\\n12.12\\n\\n12.79\\n\\n18.45\\n\\n(2) RAG-original-QA+R (DPR-adapted)\\n\\n7.36\\n\\n17.91\\n\\n22.39\\n\\n30.87\\n\\n(3) RAG-end2end-QA+R\\n\\n8.32\\n\\n19.51\\n\\n23.05\\n\\n31.23\\n\\nNews Domain\\n\\n(1) RAG-original-QA+R\\n\\n8.62\\n\\n16.46\\n\\n27.28\\n\\n39.56\\n\\n(2) RAG-original-QA+R (DPR-adapted)\\n\\n10.92\\n\\n19.44\\n\\n30.72\\n\\n41.9\\n\\n(3) RAG-end2end-QA+R\\n\\n14.08\\n\\n23.7\\n\\n39.67\\n\\n50.95\\n\\nConversation Domain\\n\\n(1) RAG-original-QA+R\\n\\n14.21\\n\\n24.62\\n\\n26.32\\n\\n36.21\\n\\n(2) RAG-original-QA+R (DPR-adapted)\\n\\n15.78\\n\\n25.47\\n\\n29.01\\n\\n40.03\\n\\n(3) RAG-end2end-QA+R\\n\\n25.95\\n\\n37.96\\n\\n49.11\\n\\n58.75\\n\\nTable 4: Comparing the effect of RAG-end2end extension, against initializing RAG-original mod- els with domain adapted DPR models prior to the ﬁne-tuning (Please check the Table 1). We use the independently domain adapted DPR models illustrated in Table 3\\n\\nWe highlight the performance improvements for both answer generation accuracy and retrieval re- call scores, where the Covid-19 domain has the largest improvements. We also compare the ﬁne- tuning RAG-end2end model with the RAG-original model, which was ﬁrst initialized with the domain- adapted DPR models (Compare (2) and (3) in Ta- ble 4). This comparison shows that RAG-end2end training mechanism can outperform the RAG- original mechanism that uses a domain-adapted DPR. The results further highlight the importance of RAG-end2end mechanism in domain adaptation where we do not need to train the DPR model sepa- rately.\\n\\n5 Discussion\\n\\n5.1 Role of retriever in domain adaptation\\n\\nAs the results suggest, the retriever plays an es- sential role in domain adaptation for open-domain QA. It is clear that RAG-end2end training improves the results since it can update the embeddings and the indexing of the knowledge base. Compared with the original RAG ﬁnetuning, RAG-end2end improves the performance in all datasets. The main reason for this could be that neural retrievers such as DPR, which are trained on public datasets, strug- gle to perform well on domain-speciﬁc datasets. Our results also highlight an important aspect re- lated to the performance of the stand-alone DPR for document retrieval. It shows that RAG-end2end can improve the domain adaptation of DPR better that ﬁnetuning the DPR on its own mechanism.\\n\\n5.2 Cost of end2end retriever adaptation\\n\\nIt is important to note that RAG-end2end ﬁne tun- ing can be expensive if the number of passages in the external knowledge base is large. If there are millions of passages, it would be beneﬁcial to have a dedicated number of GPUs that complete the re- encoding process. Re-indexing with the FAISS library also depends on the number of cores in the CPUs. When having access to strong enough com- putational power, it is better to use RAG-end2end since we can directly use passages in a knowledge base and question-answer pairs to train both the retriever and the reader. Then we also do not need to generate synthetic question-answer pairs related to passages that are required to train the DPR.\\n\\nAlthough the RETRO (Borgeaud et al., 2021) authors claim that frozen BERT embedding is suf- ﬁcient for retrieval augmented models, our results suggest that for domain-speciﬁc models to per- form well, a domain-adapted retriever component is beneﬁcial. In future work, it is important to ex- plore how the models like RETRO (Borgeaud et al., 2021) perform on domain-speciﬁc scenarios going beyond general-purpose datasets.\\n\\n5.3 Comparing the RAG-original with\\n\\nRAG-end2end on an In-domain dataset\\n\\nModel Name\\n\\nEM\\n\\nF1\\n\\nTop-5 Top-20\\n\\nSQUAD Open-Domain\\n\\n(1) RAG-original\\n\\n28.12\\n\\n39.42\\n\\n59.64\\n\\n72.38\\n\\n(2) RAG-end2end\\n\\n40.02\\n\\n52.63\\n\\n75.79\\n\\n85.57'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='RAG-end2end on an In-domain dataset\\n\\nModel Name\\n\\nEM\\n\\nF1\\n\\nTop-5 Top-20\\n\\nSQUAD Open-Domain\\n\\n(1) RAG-original\\n\\n28.12\\n\\n39.42\\n\\n59.64\\n\\n72.38\\n\\n(2) RAG-end2end\\n\\n40.02\\n\\n52.63\\n\\n75.79\\n\\n85.57\\n\\nTable 5: Open-Domain performance comparison between RAG-original and RAG-end2end. We used SQAD dataset in ODQA manner to conduct our experiments.\\n\\nAlthough our work is mainly focused on the domain adaptation of RAG for speciﬁc domains, we also explored whether the end2end training would improve the overall results of an in-domain dataset. Since the original RAG model uses a DPR model that is trained on a Wikipedia-based Natu- ral Questions dataset, we consider this in-domain. Although SQUAD (Rajpurkar et al., 2016) dataset is a machine comprehension dataset, we adapted the SQUAD dataset to perform ODQA. First, we extracted the contexts related to each question- answer pair and created an external knowledge base. Then we split the knowledge base into 100- words passages. Our ﬁnal knowledge base con- sists of 30K passages. As illustrated in Table 5,\\n\\nwe compared the performance of RAG-original and RAG-end2end on the tasks of answer gener- ation and retrieving correct documents. As the results suggested, RAG-end2end performs better than RAG-original even in other Wikipedia-based datasets. This could be due to RAG-end2end updat- ing the context encoder and embeddings during the training process.\\n\\n6 Conclusion and Future Work\\n\\nIn this paper, we proposed a novel extension of RAG: RAG-end2end, which, unlike RAG, does joint training of the retriever and generator for the end QA task and domain adaptation. We showed that the RAG-end2end could improve DPR perfor- mance better than ﬁne-tuning the DPR indepen- dently. This allows for the training of DPR models with QA pairs and eliminates the need for gold- standard passages related to questions. We also highlighted that the addition of a re-construction auxiliary signal further improves both the retriever and the ﬁnal answer generation accuracies. We evaluate our approach with three datasets from dif- ferent domains (COVID-19, News, and Conversa- tions), showing that RAG-end2end achieves signif- icant performance improvements in all three do- mains compared to the original RAG implementa- tion. In addition, we conducted several other exper- iments to validate our approach comprehensively. Overall, our results show that our approach is stable and generalizable across different domains. Our experiments highlight the importance of the RAG’s retriever component in domain-speciﬁc question answering.\\n\\nBased on our ﬁndings, we suggest three di- rections for future research in domain adaptation of RAG Models. Firstly, we consider it worth- while to explore RAG-end2end on other tasks like Fact Checking (Lewis et al., 2020b), Summarisa- tion (Shuster et al., 2021), and conversational re- sponse generation (Xu et al., 2021) where the orig- inal RAG has shown interesting results. Secondly, it is important to explore generative capabilities with qualitative metrics. This could be aligned with research areas like measuring factual consis- tency (Kry´sci´nski et al., 2019; Cao et al., 2022) and hallucinations (Cao et al., 2022; Shuster et al., 2021; Nie et al., 2019) of generative language mod- els. Future work could explore whether updat- ing the retriever and document embeddings during the training phase could improve factual consis-\\n\\ntency and reduce hallucinations in ﬁnal generations. Thirdly, the improvement of RAG with our exten- sion (RAG-end2end) highlights the importance of the retriever in the RAG architecture, which mo- tivates us to improve the retriever part further in future work. Also, as the statement re-construction signal acts as a good auxiliary signal, we encour- age exploring other auxiliary signals, which could improve the overall performance of RAG models.\\n\\nReferences'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='References\\n\\nChris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic qa corpora generation with roundtrip consistency. arXiv preprint arXiv:1906.05416.\\n\\nPetr Baudiš and Jan Šediv`y. 2015. Modeling of the question answering task in the yodaqa sys- tem. In International Conference of the cross- language evaluation Forum for European lan- guages, pages 222–228. Springer.\\n\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on free- base from question-answer pairs. In Proceed- ings of the 2013 conference on empirical meth- ods in natural language processing, pages 1533– 1544.\\n\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Mil- lican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2021. Improving language models by retriev- ing from trillions of tokens. arXiv preprint arXiv:2112.04426.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas- try, Amanda Askell, et al. 2020. Language mod- els are few-shot learners. Advances in neural information processing systems, 33:1877–1901.\\n\\nMeng Cao, Yue Dong, and Jackie Chi Kit Che- ung. 2022. Hallucinated but factual! inspect- ing the factuality of hallucinations in abstrac- tive summarization. In Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 3340–3354.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language un- derstanding. arXiv preprint arXiv:1810.04805.\\n\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre- training. arXiv preprint arXiv:2002.08909.\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28:1693– 1701.\\n\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset arXiv preprint for reading comprehension. arXiv:1705.03551.\\n\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.\\n\\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer lan- guage model for controllable generation. arXiv preprint arXiv:1909.05858.\\n\\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2021. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566.\\n\\nWojciech Kry´sci´nski, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Evaluating the factual consistency of abstractive text sum- marization. arXiv preprint arXiv:1910.12840.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Ja- cob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466.\\n\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. arXiv preprint arXiv:1906.00300.\\n\\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettle- moyer. 2020a. Pre-training via paraphrasing. arXiv preprint arXiv:2006.15020.'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettle- moyer. 2020a. Pre-training via paraphrasing. arXiv preprint arXiv:2006.15020.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\\n\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. 2020b. Retrieval- augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401.\\n\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2020c. Question and answer test-train overlap in open-domain question answering datasets. arXiv preprint arXiv:2008.02637.\\n\\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. Paq: 65 million probably-asked questions and what you can do with them. arXiv preprint arXiv:2102.07033.\\n\\nEdward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. arXiv preprint cs/0205028.\\n\\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2020. Zero-shot neural re- trieval via domain-targeted synthetic query gen- eration. arXiv preprint arXiv:2004.14503.\\n\\nSewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi, Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palomaki, et al. 2021. Neurips 2020 efﬁcientqa competition: Systems, anal- arXiv preprint yses and lessons learned. arXiv:2101.00133.\\n\\nTimo Moller, Anthony Reina, Raghavan Jayaku- mar, and Malte Pietsch. 2020. COVID-QA: A question answering dataset for COVID-19. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online. Association for Computational Linguistics.\\n\\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efﬁcient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Stor- age and Analysis, pages 1–15.\\n\\nFeng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew Lin. 2019. A simple recipe to- wards reducing hallucination in neural surface realisation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2673–2679.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ ques- tions for machine comprehension of text. arXiv preprint arXiv:1606.05250.\\n\\nStephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc.\\n\\nDevendra Singh Sachan, Mostofa Patwary, Moham- mad Shoeybi, Neel Kant, Wei Ping, William L Hamilton, and Bryan Catanzaro. 2021. End- to-end training of neural retrievers for open- arXiv preprint domain question answering. arXiv:2101.00408.\\n\\nSiamak Shakeri, Cicero Nogueira dos Santos, Henry Zhu, Patrick Ng, Feng Nan, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. 2020. End- to-end synthetic data generation for domain adaptation of question answering systems. arXiv preprint arXiv:2010.06028.\\n\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval aug- mentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567.'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval aug- mentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567.\\n\\nDevendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-to-end training of multi-document reader and retriever for open-domain question answering. Advances in Neural Information Processing Systems, 34.\\n\\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2016. Newsqa: A ma- chine comprehension dataset. arXiv preprint arXiv:1611.09830.\\n\\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn Funk, Rodney Kinney, Ziyang Liu, William Merrill, et al. 2020. Cord-19: The covid-19 open research dataset. ArXiv.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface’s trans- formers: State-of-the-art natural language pro- cessing. arXiv preprint arXiv:1910.03771.\\n\\nChien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021a. Controllable abstractive dialogue summariza- tion with sketch supervision. arXiv preprint arXiv:2105.14064.\\n\\nChien-Sheng Wu, Andrea Madotto, Wenhao Liu, Pascale Fung, and Caiming Xiong. 2021b. Qa- conv: Question answering on informative con- versations. arXiv preprint arXiv:2105.06912.\\n\\nJing Xu, Arthur Szlam, and Jason Weston. 2021. Beyond goldﬁsh memory: Long-term arXiv preprint open-domain conversation. arXiv:2107.07567.\\n\\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for open- domain question answering. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 2013–2018.\\n\\nA Appendix'),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"Test Set QuestionRAG-original retrived passagesRAG-end2end retrived passagesLabelRAG-original predictionRAG-end2end predictionWhere does the Kiwi girl that Darren T Maloney spoke to on the phone commute from ?PETER KENYON, BYLINE: MARY O'CONNOR, It will be very hard, apart, you know, to kind of ensure free travel. And we have got used to it. I mean, I remember the time when you traveled to Northern Ireland when there was two and three hours' wait, you know, over and back across the border. And I wouldn't like to see that happen. I don't think it would help trade. <end> PETER KENYON, BYLINE: But on this day, at least, the economic arguments weren't entirely persuasive. As Bernadette Wray finishes her lunch, she says she's got nothing against immigrants, being one PETER KENYON, BYLINE: MARY O'CONNOR, It will be very hard, apart, you know, to kind of ensure free travel. And we have got used to it. I mean, I remember the time when you traveled to Northern Ireland when there was two and three hours' wait, you know, over and back across the border. And I wouldn't like to see that happen. I don't think it would help trade. <end> PETER KENYON, BYLINE: But on this day, at least, the economic arguments weren't entirely persuasive. As Bernadette Wray finishes her lunch, she says she's got nothing against immigrants, being one really appreciate it. <end> MARTINA DELVIN: Pleasure. <end> STEVE INSKEEP, HOST: She is an author and columnist in Dublin. really appreciate it. <end> MARTINA DELVIN: Pleasure. <end> STEVE INSKEEP, HOST: She is an author and columnist in Dublin. STEVE INSKEEP, HOST: So by emphasizing the divide in this historically divided island, it would threaten the reignition of violence on the island there. That's what you're saying. <end> MARTINA DELVIN: That's right. And there were 30 years of violence, and they impacted on all sorts of ways on daily life and on people's ability to earn a living - led to emigration - all sorts. I mean, I grew up in Northern Ireland during The Troubles. And the - you know, the island was divided in the most arbitrary way. So you'll get, for example, a petrol station withDarren T Maloney Susan Did I mention Wilson Coming back to Houstonnever for any length given his love for NYC Yup 6 weeks Mixed bc everytime I get dug in I get moved Actually I am establishing a London base now and am happy with that but my project is on hold and I am a bit anxious The Kiwi girl sits next to me and has to commute from what seems like N Africa to get to work so she is sick of the commute Well that's all for now I will see you Darren T Maloney Susan Did I mention Wilson Coming back to Houstonnever for any length given his love for NYC Yup 6 weeks Mixed bc everytime I get dug in I get moved Actually I am establishing a London base now and am happy with that but my project is on hold and I am a bit anxious The Kiwi girl sits next to me and has to commute from what seems like N Africa to get to work so she is sick of the commute Well that's all for now I will see you PatriceLMims Hey what's Happening Just wanted to let you know that today is my Birthday We're going yo go to the KiCi and Jo Jo Concert tonight at the Arena Theater should be very good Also did you hear from Robin that Anna's daughter Dana is in the hospital I'm going to call up there this afternoon and see how she's doing I'm assuming her last name is Brown Also I hooked Cynthia Patterson up with one of my customers A divorced brother from S Carolina Girlm she called me this morning and she was so giddy PatriceLMims Hey what's Happening Just wanted to let you know that today is my Birthday We're going yo go to the KiCi and Jo Jo Concert tonight at the Arena Theater should be very good Also did you hear from Robin that Anna's daughter Dana is in the hospital I'm going to call up there this afternoon and see how she's doing I'm assuming her last name is Brown Also I hooked Cynthia Patterson up with one of my customers A divorced brother from S Carolina\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"I'm going to call up there this afternoon and see how she's doing I'm assuming her last name is Brown Also I hooked Cynthia Patterson up with one of my customers A divorced brother from S Carolina Girlm she called me this morning and she was so giddy time last year maybe somebody who has to work today or who is simply stuck at the airport Our phone number is 8009898255 or you can email us at talknprorg Or you can also join the conversation at our Website That's nprorg and just click on TALK OF THE NATION end JOHN DONVAN HOST Well when we asked this very same question last year Alita Corneliusph emailed us then with a reply She wrote then Today Thanksgiving my daughter is not at my table and I miss her horribly But the way society is today children must move to citiesN AfricaNew Zealandwhat seems like N Africa to get to workWhy Brian Kerrigan apologize to Deutsche bank?JUSTICE GINSBURG: And the remedies being injunctive and declaratory. <end> MR. LAMKEN (PETITIONER): A -- a form of specific relief. Generally they have the authority to effectively go in and revise the decision below, but the remedies ordinarily do not include monetary or compensatory relief I should say. <end> JUSTICE GINSBURG: Are you saying that it's parallel to what APA review of an agency decision would be? <end> MR. LAMKEN (PETITIONER): It's very much like that. The remand rule that this Court normally requires in the APA context is not so strictly observed in the context of -- of review JUSTICE GINSBURG: And the remedies being injunctive and declaratory. <end> MR. LAMKEN (PETITIONER): A -- a form of specific relief. Generally they have the authority to effectively go in and revise the decision below, but the remedies ordinarily do not include monetary or compensatory relief I should say. <end> JUSTICE GINSBURG: Are you saying that it's parallel to what APA review of an agency decision would be? <end> MR. LAMKEN (PETITIONER): It's very much like that. The remand rule that this Court normally requires in the APA context is not so strictly observed in the context of -- of review JUSTICE GINSBURG: That's why one has preclusion because you are giving respect, full faith and credit, to a decision elsewhere. That's what preclusion doctrine is all about. We respect the judgment of the court that rendered it. We, therefore, give it full faith and credit. That's what preclusion doctrine is about, is about respect and credit. Isn't that so? <end> MR. CASTANIAS (RESPONDENT): That's -- that's -- that is -- that is generally right, Justice Ginsburg, but at the same time, there -- we all agree -- Exxon Mobil, SABIC, and the decisions of this Court -- that there has JUSTICE GINSBURG: That's why one has preclusion because you are giving respect, full faith and credit, to a decision elsewhere. That's what preclusion doctrine is all about. We respect the judgment of the court that rendered it. We, therefore, give it full faith and credit. That's what preclusion doctrine is about, is about respect and credit. Isn't that so? <end> MR. CASTANIAS (RESPONDENT): That's -- that's -- that is -- that is generally right, Justice Ginsburg, but at the same time, there -- we all agree -- Exxon Mobil, SABIC, and the decisions of this Court -- that there has JUSTICE GINSBURG: Then he -- then he can publish his -- he can publish his dissent, just as a Tax Court judge can? <end> MR. HUNGAR (RESPONDENT): No, but he can preclude the Tax Court judge from doing what the Tax Court judge did in this case, which is simply adopting his report. If the -- if the special trial judge refuses to change his report -- <end> JUSTICE GINSBURG: But then we still won't know what his report is. Yes, he can say, I won't sign this. Tax Court says, fine. This rule says I can reject your findingsBrian Kerrigan PRIVILEGED AND CONFIDENTIAL ATTORNEY CLIENT COMMUNICATION I spoke with Deutsche bank immediately after my conversation with Dan Lyons I apologize as I must not have communicated clearly that I understood the\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"AND CONFIDENTIAL ATTORNEY CLIENT COMMUNICATION I spoke with Deutsche bank immediately after my conversation with Dan Lyons I apologize as I must not have communicated clearly that I understood the importance of this issue from a legal point of view My intent on having language proposed was not to concede to any proposed language by Deustche but to determine if it was possible to be assured of bringing in a significant commitment to the syndication of the transaction as well as not create any potential legal issues As this was not the case I told Brian Kerrigan PRIVILEGED AND CONFIDENTIAL ATTORNEY CLIENT COMMUNICATION I spoke with Deutsche bank immediately after my conversation with Dan Lyons I apologize as I must not have communicated clearly that I understood the importance of this issue from a legal point of view My intent on having language proposed was not to concede to any proposed language by Deustche but to determine if it was possible to be assured of bringing in a significant commitment to the syndication of the transaction as well as not create any potential legal issues As this was not the case I told Specifically the current definition of Indemnifiable Tax covers both present and future tax as evidenced in the definition Tax and thus necessarily should protect the counterparty in the case of a Change in Tax Law Moreover Deutsche Bank's amendment to Indemnifiable Tax potentially subjects us to risk that I don't feel is in our best interest to assume because we don't have any control over the counterparty's activities Bottom line I would reject the amendment Please let me know if you would like me to speak to their tax counsel Rhett Jackson EB 4680 7138534718 Specifically the current definition of Indemnifiable Tax covers both present and future tax as evidenced in the definition Tax and thus necessarily should protect the counterparty in the case of a Change in Tax Law Moreover Deutsche Bank's amendment to Indemnifiable Tax potentially subjects us to risk that I don't feel is in our best interest to assume because we don't have any control over the counterparty's activities Bottom line I would reject the amendment Please let me know if you would like me to speak to their tax counsel Rhett Jackson EB 4680 7138534718 been intercepted or amended please tell us as soon as possible end You're right offtopicitis has got me again Sorry end Paul Davis none really but Freud would forgive the association of thought from Cynthia's posting can you errplease earnest smiles Paul end Charles McCathieNevile Naturally you are forgiven Actually I think there are some potentially interesting legal ramifications if passing the cost of accessibility to the customer is found to be legal then it suggests a particular way to pay for what appeared to be already legally necessary accessibility improvements at least in Australia and thenot have communicated clearlyDeutsche Bank apologize for putting unnecessary stress on the U.S. housing marketnot have communicated clearly that he understood the importance of this issue from a legal point of viewWhich person made his most famous speech on the steps of the Lincoln Memorial?Oliver Cromwell, who beheaded him, on the other. I don't know if you have to endorse one or the other. <end> GEN. ABBOTT (RESPONDENT): Well, Justice Kennedy, I believe that there is a very meaningful difference between this Court's standards of an endorsement and what a State or the nation may do with regard to commemoration. As an easy example, on the National Mall, there is, of course, the Lincoln Memorial and in the Lincoln Memorial, there is text from the King James version of the Bible. The nation commemorates and acknowledges Lincoln and what he has said. But by individual will be engaged in a -- in a situation where they will be endangered. And I think that was certainly true of William Lloyd in the Totten case. When he crossed southern lines, he was very much endangered, and that's something that wasn't lost on\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"they will be endangered. And I think that was certainly true of William Lloyd in the Totten case. When he crossed southern lines, he was very much endangered, and that's something that wasn't lost on President Lincoln. In footnote 3 of our opening brief, we have a quotation from President Lincoln about the inherent dangers of spies crossing lines and the need for secrecy to protect that. So that's why I think that claim is properly understood as not being covered by the Tucker Act and not being required individual will be engaged in a -- in a situation where they will be endangered. And I think that was certainly true of William Lloyd in the Totten case. When he crossed southern lines, he was very much endangered, and that's something that wasn't lost on President Lincoln. In footnote 3 of our opening brief, we have a quotation from President Lincoln about the inherent dangers of spies crossing lines and the need for secrecy to protect that. So that's why I think that claim is properly understood as not being covered by the Tucker Act and not being required individual will be engaged in a -- in a situation where they will be endangered. And I think that was certainly true of William Lloyd in the Totten case. When he crossed southern lines, he was very much endangered, and that's something that wasn't lost on President Lincoln. In footnote 3 of our opening brief, we have a quotation from President Lincoln about the inherent dangers of spies crossing lines and the need for secrecy to protect that. So that's why I think that claim is properly understood as not being covered by the Tucker Act and not being required with President Lincoln to engage in espionage activities in the south. And this Court held that when the estate of -- of Mr. Lloyd came to seek compensation from a court, that there was no judicial remedy to enforce that alleged agreement, and the remedy, if any, lay with the President's contingent fund. <end> JUSTICE KENNEDY: I -- I'd like your help on this. Your interpretation of Totten -- does it say that there is just no actionable contract, or does it say there's no jurisdictions like political question? I mean, you win under any of those theories, if weend NEAL CONAN HOST Many other's visited the new King memorial on the National Mall and millions around the country took part in a national day of service Every year on this program we return to the march on Washington and probably King's most famous speech Here's Martin Luther King Jr August 28 1963 on the steps of the Lincoln Memorial end NEAL CONAN HOST MARTIN LUTHER KING JR I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation And five score years end NEAL CONAN HOST Many other's visited the new King memorial on the National Mall and millions around the country took part in a national day of service Every year on this program we return to the march on Washington and probably King's most famous speech Here's Martin Luther King Jr August 28 1963 on the steps of the Lincoln Memorial end NEAL CONAN HOST MARTIN LUTHER KING JR I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation And five score years Thank God almighty we are free at last end NEAL CONAN HOST Martin Luther King Jr on the steps of the Lincoln Memorial August 28 1963 This is TALK OF THE NATION from NPR News I'm Neal Conan in Washington Thank God almighty we are free at last end NEAL CONAN HOST Martin Luther King Jr on the steps of the Lincoln Memorial August 28 1963 This is TALK OF THE NATION from NPR News I'm Neal Conan in Washington NEAL CONAN HOST He came and he gave a public lecture That lecture was in Mary Dodd Brown Memorial Chapel And finally I had the great honor of being a the temporary guardian of his gift to Lincoln University He gave a collection of books manuscripts and other memorabilia which is housed in Langston Hughes\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"finally I had the great honor of being a the temporary guardian of his gift to Lincoln University He gave a collection of books manuscripts and other memorabilia which is housed in Langston Hughes Memorial Library today end NEAL CONAN HOST It must have been I've read that in fact he liked to go to the dorms and hang out with the students and tell stories It must have been extraordinary to have that opportunity end LANGSTONMartin Luther King Jr.LincolnMartin Luther King JrWhat library is recommended for sending mail?NEAL CONAN, HOST: So if you wrote a check, for example, to the United Way, you have no idea where that money is necessarily going - for good causes, you suspect - but that good cause may be the heating bill at the headquarters. <end> LAURA VANDERKAM: Well, certainly that's true. I mean, all nonprofits do have some need for overhead to run their operations. But generally, you know, this is the way professional philanthropy has worked, is that you trust that the experts who are running these philanthropies know what the most urgent causes are, know what is the NEAL CONAN, HOST: So if you wrote a check, for example, to the United Way, you have no idea where that money is necessarily going - for good causes, you suspect - but that good cause may be the heating bill at the headquarters. <end> LAURA VANDERKAM: Well, certainly that's true. I mean, all nonprofits do have some need for overhead to run their operations. But generally, you know, this is the way professional philanthropy has worked, is that you trust that the experts who are running these philanthropies know what the most urgent causes are, know what is the my usual beat. And I know that cities everywhere in the United States are struggling to get more things recycled, get paper out of the landfill, and this is an enormous burden on cities and on taxpayers because much of this mail is never even looked at. <end> NEAL CONAN, HOST: Not even looked at. <end> ELISABETH ROSENTHAL: Yup. <end> NEAL CONAN, HOST: And as you look at what is - the trend is unmistakable. I mean, people do request that their - I guess their credit card bills, most of us, show up on paper. We tend to pay my usual beat. And I know that cities everywhere in the United States are struggling to get more things recycled, get paper out of the landfill, and this is an enormous burden on cities and on taxpayers because much of this mail is never even looked at. <end> NEAL CONAN, HOST: Not even looked at. <end> ELISABETH ROSENTHAL: Yup. <end> NEAL CONAN, HOST: And as you look at what is - the trend is unmistakable. I mean, people do request that their - I guess their credit card bills, most of us, show up on paper. We tend to pay my usual beat. And I know that cities everywhere in the United States are struggling to get more things recycled, get paper out of the landfill, and this is an enormous burden on cities and on taxpayers because much of this mail is never even looked at. <end> NEAL CONAN, HOST: Not even looked at. <end> ELISABETH ROSENTHAL: Yup. <end> NEAL CONAN, HOST: And as you look at what is - the trend is unmistakable. I mean, people do request that their - I guess their credit card bills, most of us, show up on paper. We tend to payKaminski Vince Kaminski Celeste I am forwarding you a letter from Prof Duane Seppi from Carnegie Mellon University I have known Duane for many years and I know that he does not make his recommendations without very good reasons I would recommend looking at John Gordon as a very strong candidate I think he will make a terrific contribution to Enron Vince Kaminski Vince Kaminski Celeste I am forwarding you a letter from Prof Duane Seppi from Carnegie Mellon University I have known Duane for many years and I know that he does not make his recommendations without very good reasons I would recommend looking at John Gordon as a very strong candidate I think he will make a terrific contribution to Enron Vince Kaminski Vince Kaminski Celeste I am forwarding you a\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"very good reasons I would recommend looking at John Gordon as a very strong candidate I think he will make a terrific contribution to Enron Vince Kaminski Vince Kaminski Celeste I am forwarding you a letter from Prof Duane Seppi from Carnegie Mellon University I have known Duane for many years and I know that he does not make his recommendations without very good reasons I would recommend looking at John Gordon as a very strong candidate I think he will make a terrific contribution to Enron Vince Sheridan Titman Dear Ben I enjoyed meeting with you I sent a letter recommending you to our Dean in charge of the MBA program Let me know if you hear anything from the people in the admissions office I hope things work out and I have the opportunity to have you participate in our energy finance program By the way I like the idea of bringing in various industry leaders I currently do this in my class but it might make sense to consider ways to get first year students involved in this activity regards Sheridan Sheridan Sheridan Titman Dear Ben I enjoyed meeting with you I sent a letter recommending you to our Dean in charge of the MBA program Let me know if you hear anything from the people in the admissions office I hope things work out and I have the opportunity to have you participate in our energy finance program By the way I like the idea of bringing in various industry leaders I currently do this in my class but it might make sense to consider ways to get first year students involved in this activity regards Sheridan Sheridansmtplibe-mailPostal ServiceHow many payday loan stores are there?usually work, Alex. They buy the right to collect outstanding debts at a discount and they get a share of the take. The difference here is that some of these hospitals have started to auction off their debt on a couple of online websites that have been developed just for this purpose and that turns the debt collection firms into bidders. The hospital may get more money but the flip side of that is that buying a debt can become more expensive for the collectors and this arrangement has the collectors either buying the debt outright or providing a guarantee usually work, Alex. They buy the right to collect outstanding debts at a discount and they get a share of the take. The difference here is that some of these hospitals have started to auction off their debt on a couple of online websites that have been developed just for this purpose and that turns the debt collection firms into bidders. The hospital may get more money but the flip side of that is that buying a debt can become more expensive for the collectors and this arrangement has the collectors either buying the debt outright or providing a guarantee also subscriber newsletters that we are giving to our paying customers that help them understand the back-story behind some of the more significant news pieces that we are reporting on. also subscriber newsletters that we are giving to our paying customers that help them understand the back-story behind some of the more significant news pieces that we are reporting on. from Florida. It's a huge number. It's obviously decreasing now. And so now drug addicts and abusers, when they can get their pills from doctors, or if they don't buy them on the street, they are going to have to turn to pharmacies. But with that said, at least one pharmacy chain is implementing some changes. CVS recently sent a letter to what it deemed high-dispensing doctors, telling them that it was no longer going to fill their prescriptions for pain killers and other scheduled substances.McDonald's end STACEY VANEK SMITH BYLINE Actually there are more payday loan stores than McDonald's or Starbucks There are nearly 18000 payday loan stores in this country right now end RONALD MANN So I think what you really have to see is to step back and say or ask why are there so many people in our economy that are struggling so hard end STACEY VANEK SMITH BYLINE People like Amy Marineau\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"So I think what you really have to see is to step back and say or ask why are there so many people in our economy that are struggling so hard end STACEY VANEK SMITH BYLINE People like Amy Marineau end AMY MARINEAU The turning point for me was having to at 43 live with my mother again and not being able to take care of our family McDonald's end STACEY VANEK SMITH BYLINE Actually there are more payday loan stores than McDonald's or Starbucks There are nearly 18000 payday loan stores in this country right now end RONALD MANN So I think what you really have to see is to step back and say or ask why are there so many people in our economy that are struggling so hard end STACEY VANEK SMITH BYLINE People like Amy Marineau end AMY MARINEAU The turning point for me was having to at 43 live with my mother again and not being able to take care of our family RONALD MANN I mean these are products that are there's a fair chance people aren't going to be able to pay them back end STACEY VANEK SMITH BYLINE Ronald says that is exactly why about 20 states have either banned payday loans entirely or really restricted them end CARDIFF GARCIA BYLINE On the other hand more than 30 states don't really have restrictions at all on payday lending And in those states payday lending has gotten huge or you might say supersized end RONALD MANN The number of payday loan stores is about the same as the number of RONALD MANN I mean these are products that are there's a fair chance people aren't going to be able to pay them back end STACEY VANEK SMITH BYLINE Ronald says that is exactly why about 20 states have either banned payday loans entirely or really restricted them end CARDIFF GARCIA BYLINE On the other hand more than 30 states don't really have restrictions at all on payday lending And in those states payday lending has gotten huge or you might say supersized end RONALD MANN The number of payday loan stores is about the same as the number of NEAL CONAN HOST This is TALK OF THE NATION I'm Neal Conan in Washington Yesterday the United States Postal Service announced plans to close about 250 processing centers lay off about 28000 workers and accept slower first class service as a consequence As many as 2000 post offices could close and Saturday delivery could be on the block as well end NEAL CONAN HOST All those cuts could reduce losses maybe even put the USPS in the black but when your mailbox is stuffed with direct mail ads some raise what was once unthinkable Has the post office outlived its18000there are payday loan stores in every State in the Nationnearly 18000Which programming language implements provide transformers according to Kimery?was not a single overlapping step between that new process and Morse's process. <end> JUSTICE BREYER: Yes. And we here apply the correlation to any homocysteine test, any one here, any one in the future, any one that any mind might impend. What's the difference? <end> MR. HUNGAR (None): Well, the difference is between claiming a -- claiming all methods of achieving a particular result and claiming one process for achieving that particular result and then as one claiming any means of doing one particular step of that process. <end> JUSTICE BREYER: I apply electricity to all methods of putting was not a single overlapping step between that new process and Morse's process. <end> JUSTICE BREYER: Yes. And we here apply the correlation to any homocysteine test, any one here, any one in the future, any one that any mind might impend. What's the difference? <end> MR. HUNGAR (None): Well, the difference is between claiming a -- claiming all methods of achieving a particular result and claiming one process for achieving that particular result and then as one claiming any means of doing one particular step of that process. <end> JUSTICE BREYER: I apply electricity to all methods of putting in an appropriate way, what was at issue there. There was a disagreement about the content of the allegations. <end> MS. ROBIN-VERGEER\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"JUSTICE BREYER: I apply electricity to all methods of putting in an appropriate way, what was at issue there. There was a disagreement about the content of the allegations. <end> MS. ROBIN-VERGEER (RESPONDENT): I don't think it's important, for, maybe, purposes of this, to iron this out, but I -- respectfully, I don't agree with that characterization, because, even in the resolution of the grievance internally, the -- what they found in the grievance was that they took no adverse action against him because of what he said -- <end> JUSTICE BREYER: That doesn't -- <end> MS. ROBIN-VERGEER (RESPONDENT): -- in connection with this case. <end> JUSTICE BREYER: That isn't in an appropriate way, what was at issue there. There was a disagreement about the content of the allegations. <end> MS. ROBIN-VERGEER (RESPONDENT): I don't think it's important, for, maybe, purposes of this, to iron this out, but I -- respectfully, I don't agree with that characterization, because, even in the resolution of the grievance internally, the -- what they found in the grievance was that they took no adverse action against him because of what he said -- <end> JUSTICE BREYER: That doesn't -- <end> MS. ROBIN-VERGEER (RESPONDENT): -- in connection with this case. <end> JUSTICE BREYER: That isn't in an appropriate way, what was at issue there. There was a disagreement about the content of the allegations. <end> MS. ROBIN-VERGEER (RESPONDENT): I don't think it's important, for, maybe, purposes of this, to iron this out, but I -- respectfully, I don't agree with that characterization, because, even in the resolution of the grievance internally, the -- what they found in the grievance was that they took no adverse action against him because of what he said -- <end> JUSTICE BREYER: That doesn't -- <end> MS. ROBIN-VERGEER (RESPONDENT): -- in connection with this case. <end> JUSTICE BREYER: That isn'tKimbery I looked at the source code for Racketâ€™s require and provide and I examined %require and %provide a little end Kimbery Hereâ€™s what I learned require transformers and provide pretransformers are implemented in Racket Provide transformers are implemented in C end Kimbery I think provide transformers are pretty restricted because they have access to the bindings exported by allfromout and things like that but in order to know that the module needs to know which bindings are shadowed by the module body So provide transformers are essentially the very last step of macro transformation in a moduleâ€™s expansion end Kimbery I looked at the source code for Racketâ€™s require and provide and I examined %require and %provide a little end Kimbery Hereâ€™s what I learned require transformers and provide pretransformers are implemented in Racket Provide transformers are implemented in C end Kimbery I think provide transformers are pretty restricted because they have access to the bindings exported by allfromout and things like that but in order to know that the module needs to know which bindings are shadowed by the module body So provide transformers are essentially the very last step of macro transformation in a moduleâ€™s expansion end Kimbery I looked at the source code for Racketâ€™s require and provide and I examined %require and %provide a little end Kimbery Hereâ€™s what I learned require transformers and provide pretransformers are implemented in Racket Provide transformers are implemented in C end Kimbery I think provide transformers are pretty restricted because they have access to the bindings exported by allfromout and things like that but in order to know that the module needs to know which bindings are shadowed by the module body So provide transformers are essentially the very last step of macro transformation in a moduleâ€™s expansion end Chantelle right that makes sense end Kimbery yes require transformers are essentially just ordinary expanders that require looks up with syntaxlocalvalue end Kimbery require transformers arenâ€™t special in any way\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"that makes sense end Kimbery yes require transformers are essentially just ordinary expanders that require looks up with syntaxlocalvalue end Kimbery require transformers arenâ€™t special in any way only provide transformers are end Chantelle right and localprovide is nonsensical so there's no worries there end Chantelle would it make sense to have some way of delaying provide expansion in the same way that %expression delays things so that provide transformers didn't need their importing require to be in a particular place end Chantelle my guess is probably not because complicated reasons end Kimbery thatâ€™s exactly what provide transformers Chantelle right that makes sense end Kimbery yes require transformers are essentially just ordinary expanders that require looks up with syntaxlocalvalue end Kimbery require transformers arenâ€™t special in any way only provide transformers are end Chantelle right and localprovide is nonsensical so there's no worries there end Chantelle would it make sense to have some way of delaying provide expansion in the same way that %expression delays things so that provide transformers didn't need their importingrequire tobeinaparticularplaceendChantellemyguessisprobablynotbecausecomplicatedreasonsendKimberythatâ€™sexactlywhatprovidetransformerscPythonCHow much in debt does Gustavo Arellano claim Hillary Clinton is in and should leave?I think she should go. We need to unite this party. And you know I've said it before, I think she's the better candidate. I think that Barack is a better movement. <end> MADELEINE BRAND, host: And Gustavo? <end> Mr. GUSTAVO ARELLANO (Columnist, AskaMexican.net): This campaign is in tatters. She has to leave. She's what, 31 million dollars in debt, she's increasingly becoming more and more bickering, throwing jabs at anybody who doesn't want to believe in her gospel that she was meant to be president. And I'm tired of Hillary Clinton. And I know nowadays we expect our candidates I think she should go. We need to unite this party. And you know I've said it before, I think she's the better candidate. I think that Barack is a better movement. <end> MADELEINE BRAND, host: And Gustavo? <end> Mr. GUSTAVO ARELLANO (Columnist, AskaMexican.net): This campaign is in tatters. She has to leave. She's what, 31 million dollars in debt, she's increasingly becoming more and more bickering, throwing jabs at anybody who doesn't want to believe in her gospel that she was meant to be president. And I'm tired of Hillary Clinton. And I know nowadays we expect our candidates RACHEL MARTIN, HOST: Meanwhile, federal workers are expected to miss their second paycheck tomorrow. I mean, people are really suffering in this moment. There is political pressure on both sides. The Senate's got these two bills that they're going to bring up. They're both expected to fail. So where's the opening to end this? <end> DOMENICO MONTANARO, BYLINE: Well, the president has said, so far, that he's not budging on a wall. Democrats say they have a reason for not caving either. Take a listen to what House Speaker Nancy Pelosi said about that yesterday. <end> NANCY PELOSI: There is RACHEL MARTIN, HOST: Meanwhile, federal workers are expected to miss their second paycheck tomorrow. I mean, people are really suffering in this moment. There is political pressure on both sides. The Senate's got these two bills that they're going to bring up. They're both expected to fail. So where's the opening to end this? <end> DOMENICO MONTANARO, BYLINE: Well, the president has said, so far, that he's not budging on a wall. Democrats say they have a reason for not caving either. Take a listen to what House Speaker Nancy Pelosi said about that yesterday. <end> NANCY PELOSI: There is morning. Hey, Domenico. <end> DOMENICO MONTANARO, BYLINE: Good morning, Rachel. <end> RACHEL MARTIN, HOST: So President Trump, as we know, not someone who backs down easily, but I guess he didn't really have a choice - right? - if Nancy Pelosi said he\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content=\"Good morning, Rachel. <end> RACHEL MARTIN, HOST: So President Trump, as we know, not someone who backs down easily, but I guess he didn't really have a choice - right? - if Nancy Pelosi said he is not invited to her House.Dylan Windham JeffDasovich The entire phone bill is 300 pesos Yes please fax to me And please email me rest of expenses when you get a chance so I can write you a check for everything planephone golf etc etc You guys get back OK end UNKNOWN_SPEAKER you don't want to know end Dylan Windham JeffDasovich we got back ok i didn't know tulsa had such a neat airport end Dylan Windham JeffDasovich That's so weird Prentice left her car door ajar when she went to go to school yesterday morning it was dead and she Dylan Windham JeffDasovich The entire phone bill is 300 pesos Yes please fax to me And please email me rest of expenses when you get a chance so I can write you a check for everything planephone golf etc etc You guys get back OK end UNKNOWN_SPEAKER you don't want to know end Dylan Windham JeffDasovich we got back ok i didn't know tulsa had such a neat airport end Dylan Windham JeffDasovich That's so weird Prentice left her car door ajar when she went to go to school yesterday morning it was dead and she keeping them on the ticket or not It's really about the top of the ticket For all the excitement about Geraldine Ferraro that you know Mondale still lost 49 out of 50 states So I don't know what changing the vice president does so much end KEN RUDIN BYLINE But here's another thing about the scenario here If ObamaClinton ticket losses in 2012 why would Hillary Clinton be the frontrunner for 2016 Because you know you have the Andrew Cuomos and all the Democrats waiting in the wings And if they won do you think after eight years of keeping them on the ticket or not It's really about the top of the ticket For all the excitement about Geraldine Ferraro that you know Mondale still lost 49 out of 50 states So I don't know what changing the vice president does so much end KEN RUDIN BYLINE But here's another thing about the scenario here If ObamaClinton ticket losses in 2012 why would Hillary Clinton be the frontrunner for 2016 Because you know you have the Andrew Cuomos and all the Democrats waiting in the wings And if they won do you think after eight years of have any collateral The answer is no I'm dirt poor So they say well I could loan you this money but you and your family and all the work that you can do will be the collateral against that I will hold against this loan until you repay it end Dr KEVIN BALES Activist and Author Ending Slavery Now I appreciate for Americas that you actually have to kind of stretch your mind to get around that idea that you and your own work become collateral against a loan But because everything you do and all your work becomes31 million dollars31 million dollars$20 billion\"),\n",
              " Document(metadata={'source': '/content/doc/(RAG)Models_for_Open_Domain_Question_Answering.pdf'}, page_content='Figure 2: Predicted answers and retrieved passages for a set of questions from the conversational domain (Wu et al., 2021b).'),\n",
              " Document(metadata={'source': '/content/doc/Generative AI roadmap 2024.pptx'}, page_content=\"Generative AI\\n\\nRoadmap 2024\\n\\nFrom Basics to Advanced Concepts \\n\\nHere's a step-by-step guide designed for absolute beginners looking to acquire skills in Generative AI. \\n\\nThe roadmap incorporates free learning resources for both technical and tool-related skills. \\n\\nDifferent Positions or Different Levels: \\x7f \\n\\nDeveloper Level 1 or Beginner Level \\n\\nDeveloper Level 2 or Senior Level\\n\\n Researcher Leve\\n\\nThis is broken into many sections:\\n\\nPrerequisite\\n\\nFundamentals\\n\\nCore Generative Models\\n\\nDeveloping Applications Powered by LLMs\\n\\nProjects and Practical Experience\\n\\nMiscellaneous Topics\\n\\nAdvice for Productive Learning\\n\\nFAQs\\n\\nWhat is Generative AI?\\n\\nGenerative AI generates new data based on training samples. Generative models can generate Image, Text, Audio, Videos etc. data as output.\\n\\nSo generative AI is a very huge topic, \\n\\nGenerative Image Model(GANs, Various Diffusions Models\\n\\nGenerative Language Model(LLMs)\\n\\nWhen I refer to large language models, I mean natural language processing. Since NLP forms the foundation of massive language generated models(LLMs).\\n\\nUseful Learning Resource:\\n\\nGen AI Introduction Video In English: DAY - 1 | Introduction to Generative AI Community Course LIVE ! #genai #ineuron - YouTube\\n\\nGen AI Introduction Video In hindi: DAY - 1 | Introduction to Generative AI Community Course in HINDI LIVE ! #genai #ineuron - YouTube\\n\\nGen AI Foundation Free Course : Sign In | iNeuron.ai\\n\\nGenAI Introduction by Krish Naik: Webinar - Generative AI Revolution : The Future | 2024 (youtube.com)\\n\\nGenerative AI with Large Language Models:\\n\\nPrerequisite:\\n\\nProgramming Language: \\n\\nPython is the most commonly used programming language for Data Science, Machine learning and Ai domain.\\n\\nHere are some reasons why:\\n\\nCommunity Support: Active community of developers, researchers, and practitioners in the machine learning and AI domains \\n\\nLibraries and Frameworks: Many key libraries and frameworks for generative AI, such as TensorFlow, PyTorch, and Keras, have Python interfaces \\n\\nFlexibility and Productivity: Python is known for its readability, simplicity, and ease of use, making it an ideal language for rapid prototyping and experimentation \\n\\nData Analysis and Visualization: Python is widely used in data science, and it has excellent support for data manipulation and visualisation libraries, such as NumPy, Pandas, and Matplotlib.\\n\\nTopics to Learn- \\n\\nVariables, Numbers, Strings \\n\\nLists, Dictionaries, Sets, tuples \\n\\nIf condition, for loop\\n\\nFunctions, Lambda Functions \\n\\nModules (pip install)\\n\\nRead, Write files\\n\\nException handling \\n\\nClasses, Objects \\n\\nUseful Learning Resource: Sign In | iNeuron.ai\\n\\nSQL Database(Optional)\\n\\nDeep learning projects often involve working with large volumes of unstructured data, such as images, text, or audio. In any cases, traditional SQL databases may not be the primary choice for storing such unstructured data, and other types of data storage solutions might be more appropriate But for the machine learning project it is important to.\\n\\nMy Preference: MySql or Sqlite3\\n\\nSQL Topics to Learn:\\n\\nBasics of Relational Databases \\n\\nBasic Queries: SELECT, WHERE LIKE, DISTINCT, BETWEEN, GROUP BY, ORDER BY\\n\\nAdvanced Queries: CTE, Subqueries, Window Functions Joins: Left, Right, Inner, Full\\n\\nUseful Learning Resources\\n\\nSQL Complete Playlist - https://bit.ly/3vBFMFë \\n\\nNoSQL Database:\\n\\nThe need for a NoSQL database in a deep learning project depends on the nature of your data and the specific requirements of your project. Deep learning projects often involve working with large volumes of unstructured data, such as images, text, or audio. In many cases, NoSQL databases are used to store and manage such unstructured data efficiently. \\n\\nReason by you should Nosql Database: \\n\\nScalability and Flexibility:\\n\\nVariety of Data Types:\\n\\nReal-time Data Ingestion:\\n\\nDistributed Computing:\\n\\nSchema-less Design: \\n\\nMy Preference: MongoDB or CassandraDB \\n\\nUseful Learning Resource: \\n\\nhttps://www.youtube.com/watch?v=VIAcD6P_Etc\"),\n",
              " Document(metadata={'source': '/content/doc/Generative AI roadmap 2024.pptx'}, page_content='Real-time Data Ingestion:\\n\\nDistributed Computing:\\n\\nSchema-less Design: \\n\\nMy Preference: MongoDB or CassandraDB \\n\\nUseful Learning Resource: \\n\\nhttps://www.youtube.com/watch?v=VIAcD6P_Etc \\n\\nhttps://www.youtube.com/watch?v=KWoyJwqt22I&t=4498s\\n\\nFundamentals\\n\\nMath and Statistics for Data Science(optional)\\n\\nReason why we need to learn it- \\n\\nMath and statistics are fundamental for data science and AI as they draw meaningful insights from complex datasets.\\n\\nTopics to Learn in Statistics:\\n\\nTopics to Learn in Mathematics \\n\\nProbability: \\n\\nLinear Algebra : \\n\\nCalculus\\n\\nVideo of Statistics and Mathematics by Krish NaiK: learn.ineuron.ai\\n\\nBasic Deep Learning: \\n\\nUseful Learning Resource: Deep Learning Community Session by Krish Naik: Sign In | iNeuron.ai\\n\\nBasics of Natural Language Processing:\\n\\nUseful Learning Resource: NLP Community Session by Krish Naik- Sign In | iNeuron.ai\\n\\nWord Embedding Techniques:\\n\\nUseful Learning Resource: Deep Learning Community Session by krish Naik- Sign In | iNeuron.ai\\n\\nAn important concept needs to be learnt.\\n\\nTransfer Learning: from work to the learned past and applied it current challenge. \\n\\nFine-Tuning of Model: Fine-tuning refers to the process of taking a pre-trained model and further training it on a domain specific task. \\n\\nDifferent Sequence mapping: One to Many, Many to One, Many to Many \\n\\nNLP Community Session by Krish Naik: Sign In | iNeuron.ai\\n\\nCore Generative Models:\\n\\nUseful Learning Resource: \\n\\nHugging face Models Hub-  https://huggingface.co/models \\n\\nOpenAI Models-  https://platform.openai.com/docs/models \\n\\nBERT Research Paper-  https://arxiv.org/abs/1810.04805 \\n\\nGPT Research Paper - https://bit.ly/48JHwdW \\n\\nPrompt Engineering:\\n\\nVideo link from Gen AI community session: DAY- 3: Introduction to LangChain | LangChain Tutorial - YouTube\\n\\nDeveloping Applications Powered by LLMs:\\n\\nExplore Generative Model APIs:\\n\\nTopics to Learn:\\n\\nOpenAI API\\n\\nHugging Face API\\n\\nGemini API\\n\\nUseful Learning Resource: \\n\\nLink from GENAI community session course: Bitly | Page Not Found | 404\\n\\nLink from Krish sir community session for GEMINI: Bitly | Page Not Found | 404\\n\\nDocumentation Link- \\n\\nhttps://ai.google.dev/docs \\n\\nhttps://huggingface.co/docs \\n\\nhttps://platform.openai.com/docs/introduction \\n\\nFramework for Developing LLM application:\\n\\nTopics to Learn:\\n\\nLangchain\\n\\nChainlit\\n\\nLlamaIndex\\n\\nUseful Learning Resource:\\n\\nLink from GENAI community course- DAY- 3: Introduction to LangChain | LangChain Tutorial - YouTube\\n\\nDocumentation Link:\\n\\nhttps://docs.chainlit.io/get-started/overview \\n\\nhttps://docs.llamaindex.ai/en/stable/ \\n\\nhttps://python.langchain.com/docs/getastarted/introduction\\n\\nVector Databases:\\n\\nTopics to Learn:\\n\\nChormaDB\\n\\nWeaviate\\n\\nPinecone\\n\\nFAIIS\\n\\nUseful Learning Resource:\\n\\nLink from GENAI community course- DAY- 3: Introduction to LangChain | LangChain Tutorial - YouTube\\n\\nDocumentation Link:\\n\\nttps://docs.trychroma.com/ \\n\\nhttps://docs.pinecone.io/docs/overview \\n\\nhttps://weaviate.io/developers/weaviate\\n\\nTools and Framework for Web-Application :\\n\\nTopics to Learn:\\n\\nStreamlit\\n\\nGradio\\n\\nFastAPI\\n\\nFlask\\n\\nUseful Learning Resource:\\n\\nLink from GENAI community course- DAY-6: LLM Generative AI Project using OpenAI & LangChain - YouTube\\n\\nDocumentation Link:\\n\\nFastAPI (tiangolo.com)\\n\\nWelcome to Flask — Flask Documentation (3.0.x) (palletsprojects.com)\\n\\nGradio Interface Docs\\n\\nDeployment of LLM model:\\n\\nLink from GENAI community course- YouTube\\n\\nFew Advance Topics:\\n\\nProjects and Practical Experience:\\n\\nLink of iNeuron Internship Portal- https://internship.ineuron.ai/\\n\\nLink of iNeuron Job Portal- https://jobs.ineuron.ai/\\n\\nMiscellaneous Topics:\\n\\nUseful Learning Resource \\n\\nFor latest Research and News \\n\\nhttps://www.marktechpost.com/ \\n\\nhttps://paperswithcode.com/ \\n\\nhttps://aimagazine.com/\\n\\nLink of NeuroLAB: \\n\\nhttps://neurolab.ineuron.ai/\\n\\nFAQs\\n\\nDo I need a background in machine learning or deep learning to start learning generative AI?'),\n",
              " Document(metadata={'source': '/content/doc/Generative AI roadmap 2024.pptx'}, page_content='https://aimagazine.com/\\n\\nLink of NeuroLAB: \\n\\nhttps://neurolab.ineuron.ai/\\n\\nFAQs\\n\\nDo I need a background in machine learning or deep learning to start learning generative AI?\\n\\nWhile a basic understanding of machine learning concepts is beneficial, some introductory courses in machine learning can help bridge the knowledge gap. Deep learning is a fundamental part of Generative AI, and a background in it is highly recommended. Familiarity with neural networks, backpropagation, RNNs and common deep learning frameworks like TensorFlow or [yTorch is advantageous for comprehending generative models.\\n\\nHow much mathematics knowledge is required for generative AI?\\n\\nMathematics is a key component of understanding the algorithms behind generative models. Students inquire about the level of mathematical knowledge needed, with a focus on linear algebra, calculus, and probability. A foundational understanding of these mathematical concepts is beneficial.\\n\\nCan I start with generative AI without prior experience in AI or computer science?\\n\\nYes, you can start but a background in AI or computer science can provide a smoother start, there are beginner-friendly resources available to help newcomers build their skills. You can refer to this video for more details.\\n\\nMastering Generative AI with OpenAI, LangChain, and LlamaIndex Batch Launch: Big Announcement 🚀 ! Mastering Generative AI with OpenAI, LangChain, and LlamaIndex Batch Launch 🌟 - YouTube'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\\n\\nLast year COVID-19 kept us apart. This year we are finally together again.\\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\\n\\nWith a duty to one another to the American people to the Constitution.\\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny.\\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated.\\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined.\\n\\nHe met the Ukrainian people.\\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight.\\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world.\\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people.\\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.\\n\\nThey keep moving.\\n\\nAnd the costs and the threats to America and the world keep rising.\\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2.\\n\\nThe United States is a member along with 29 other nations.\\n\\nIt matters. American diplomacy matters. American resolve matters.\\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked.\\n\\nHe rejected repeated efforts at diplomacy.\\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready. Here is what we did.\\n\\nWe prepared extensively and carefully.\\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin.\\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.\\n\\nWe countered Russia’s lies with truth.\\n\\nAnd now that he has acted the free world is holding him accountable.\\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.\\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever.\\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions.\\n\\nWe are cutting off Russia’s largest banks from the international financial system.\\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.\\n\\nWe are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.\\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more.\\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.\\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains.\\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value.\\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame.\\n\\nTogether with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance.\\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine.\\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.\\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.\\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.\\n\\nFor that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia.\\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.\\n\\nAnd we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.\\n\\nPutin has unleashed violence and chaos. But while he may make gains on the battlefield – he will pay a continuing high price over the long run.\\n\\nAnd a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world.\\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers.\\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.\\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.\\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming.\\n\\nBut I want you to know that we are going to be okay.\\n\\nWhen the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger.\\n\\nWhile it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly.\\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.\\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security.\\n\\nThis is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people.\\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you.\\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people.\\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world.\\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced.\\n\\nThe pandemic has been punishing.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='We meet tonight in an America that has lived through two of the hardest years this nation has ever faced.\\n\\nThe pandemic has been punishing.\\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more.\\n\\nI understand.\\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it.\\n\\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.\\n\\nBecause people were hurting. We needed to act, and we did.\\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis.\\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.\\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance.\\n\\nAnd as my Dad used to say, it gave people a little breathing room.\\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind.\\n\\nAnd it worked. It created jobs. Lots of jobs.\\n\\nIn fact—our economy created over 6.5 Million new jobs just last year, more jobs created in one year than ever before in the history of America.\\n\\nOur economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.\\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else.\\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century.\\n\\nVice President Harris and I ran for office with a new economic vision for America.\\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up and the middle out, not from the top down.\\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well.\\n\\nAmerica used to have the best roads, bridges, and airports on Earth.\\n\\nNow our infrastructure is ranked 13th in the world.\\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that.\\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history.\\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen.\\n\\nWe’re done talking about infrastructure weeks.\\n\\nWe’re going to have an infrastructure decade.\\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.\\n\\nAs I’ve told Xi Jinping, it is never a good bet to bet against the American people.\\n\\nWe’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America.\\n\\nAnd we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice.\\n\\nWe’ll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes—so every child—and every American—has clean water to drink at home and at school, provide affordable high-speed internet for every American—urban, suburban, rural, and tribal communities.\\n\\n4,000 projects have already been announced.\\n\\nAnd tonight, I’m announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair.\\n\\nWhen we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='When we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs.\\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure.\\n\\nThere’s been a law on the books for almost a century to make sure taxpayers’ dollars support American jobs and businesses.\\n\\nEvery Administration says they’ll do it, but we are actually doing it.\\n\\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America.\\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors.\\n\\nThat’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing.\\n\\nLet me give you one example of why it’s so important to pass it.\\n\\nIf you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land.\\n\\nIt won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built.\\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”.\\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs.\\n\\nSome of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives.\\n\\nSmartphones. The Internet. Technology we have yet to invent.\\n\\nBut that’s just the beginning.\\n\\nIntel’s CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from $20 billion to $100 billion.\\n\\nThat would be one of the biggest investments in manufacturing in American history.\\n\\nAnd all they’re waiting for is for you to pass this bill.\\n\\nSo let’s not wait any longer. Send it to my desk. I’ll sign it.\\n\\nAnd we will really take off.\\n\\nAnd Intel is not alone.\\n\\nThere’s something happening in America.\\n\\nJust look around and you’ll see an amazing story.\\n\\nThe rebirth of the pride that comes from stamping products “Made In America.” The revitalization of American manufacturing.\\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas.\\n\\nThat’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country.\\n\\nGM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan.\\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year.\\n\\nPowered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight.\\n\\nAs Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.”\\n\\nIt’s time.\\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.\\n\\nInflation is robbing them of the gains they might otherwise feel.\\n\\nI get it. That’s why my top priority is getting prices under control.\\n\\nLook, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories.\\n\\nThe pandemic also disrupted global supply chains.\\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up.\\n\\nLook at cars.\\n\\nLast year, there weren’t enough semiconductors to make all the cars that people wanted to buy.\\n\\nAnd guess what, prices of automobiles went up.\\n\\nSo—we have a choice.\\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.\\n\\nI have a better plan to fight inflation.\\n\\nLower your costs, not your wages.\\n\\nMake more cars and semiconductors in America.\\n\\nMore infrastructure and innovation in America.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='I have a better plan to fight inflation.\\n\\nLower your costs, not your wages.\\n\\nMake more cars and semiconductors in America.\\n\\nMore infrastructure and innovation in America.\\n\\nMore goods moving faster and cheaper in America.\\n\\nMore jobs where you can earn a good living in America.\\n\\nAnd instead of relying on foreign supply chains, let’s make it in America.\\n\\nEconomists call it “increasing the productive capacity of our economy.”\\n\\nI call it building a better America.\\n\\nMy plan to fight inflation will lower your costs and lower the deficit.\\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan:\\n\\nFirst – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.\\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.\\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom.\\n\\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.\\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be.\\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.\\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.\\n\\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does.\\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent.\\n\\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.\\n\\nLet’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again.\\n\\nThird – cut the cost of child care. Many families pay up to $14,000 a year for child care per child.\\n\\nMiddle-class and working families shouldn’t have to pay more than 7% of their income for care of young children.\\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn’t afford child care, to be able to get back to work.\\n\\nMy plan doesn’t stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.\\n\\nAll of these will lower costs.\\n\\nAnd under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.\\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.\\n\\nI’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share.\\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.\\n\\nThat’s simply not fair. That’s why I’ve proposed a 15% minimum tax rate for corporations.\\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can’t get out of paying their taxes at home by shipping jobs and factories overseas.\\n\\nThat’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.\\n\\nSo that’s my plan. It will grow the economy and lower costs for families.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='That’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.\\n\\nSo that’s my plan. It will grow the economy and lower costs for families.\\n\\nSo what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.\\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit.\\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted.\\n\\nBut in my administration, the watchdogs have been welcomed back.\\n\\nWe’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.\\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud.\\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.\\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year.\\n\\nLowering your costs also means demanding more competition.\\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism.\\n\\nIt’s exploitation—and it drives up prices.\\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under.\\n\\nWe see it happening with ocean carriers moving goods in and out of America.\\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits.\\n\\nTonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers.\\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.\\n\\nThat ends on my watch.\\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect.\\n\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees.\\n\\nLet’s pass the Paycheck Fairness Act and paid leave.\\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty.\\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.\\n\\nAnd let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.\\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America.\\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation.\\n\\nAnd I know you’re tired, frustrated, and exhausted.\\n\\nBut I also know this.\\n\\nBecause of the progress we’ve made, because of your resilience and the tools we have, tonight I can say we are moving forward safely, back to more normal routines.\\n\\nWe’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.\\n\\nJust a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines.\\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.\\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks.\\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.\\n\\nI know some are talking about “living with COVID-19”. Tonight – I say that we will never just accept living with COVID-19.\\n\\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='We will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard.\\n\\nHere are four common sense steps as we move forward safely.\\n\\nFirst, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you’re vaccinated and boosted you have the highest degree of protection.\\n\\nWe will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children.\\n\\nThe scientists are working hard to get that done and we’ll be ready with plenty of vaccines when they do.\\n\\nWe’re also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.\\n\\nWe’ve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.\\n\\nAnd we’re launching the “Test to Treat” initiative so people can get tested at a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.\\n\\nIf you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks.\\n\\nWe’re leaving no one behind or ignoring anyone’s needs as we move forward.\\n\\nAnd on testing, we have made hundreds of millions of tests available for you to order for free.\\n\\nEven if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week.\\n\\nSecond – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants.\\n\\nIf necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.\\n\\nAnd, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed.\\n\\nI cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.\\n\\nThird – we can end the shutdown of schools and businesses. We have the tools we need.\\n\\nIt’s time for Americans to get back to work and fill our great downtowns again. People working from home can feel safe to begin to return to the office.\\n\\nWe’re doing that here in the federal government. The vast majority of federal workers will once again work in person.\\n\\nOur schools are open. Let’s keep it that way. Our kids need to be in school.\\n\\nAnd with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely.\\n\\nWe achieved this because we provided free vaccines, treatments, tests, and masks.\\n\\nOf course, continuing this costs money.\\n\\nI will soon send Congress a request.\\n\\nThe vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.\\n\\nFourth, we will continue vaccinating the world.\\n\\nWe’ve sent 475 Million vaccine doses to 112 countries, more than any other nation.\\n\\nAnd we won’t stop.\\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life.\\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.\\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together.\\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.\\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun.\\n\\nOfficer Mora was 27 years old.\\n\\nOfficer Rivera was 22.\\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='Officer Mora was 27 years old.\\n\\nOfficer Rivera was 22.\\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers.\\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\n\\nI’ve worked on these issues a long time.\\n\\nI know what works: Investing in crime prevention and community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.\\n\\nSo let’s not abandon our streets. Or choose between safety and equal justice.\\n\\nLet’s come together to protect our communities, restore trust, and hold law enforcement accountable.\\n\\nThat’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers.\\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.\\n\\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities.\\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.\\n\\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home—they have no serial numbers and can’t be traced.\\n\\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon?\\n\\nBan assault weapons and high-capacity magazines.\\n\\nRepeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued.\\n\\nThese laws don’t infringe on the Second Amendment. They save lives.\\n\\nThe most fundamental right in America is the right to vote – and to have it counted. And it’s under assault.\\n\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections.\\n\\nWe cannot let this happen.\\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.\\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.\\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\\n\\nWe can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours.\\n\\nProvide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers.\\n\\nRevise our laws so businesses have the workers they need and families don’t wait decades to reunite.\\n\\nIt’s not only the right thing to do—it’s the economically smart thing to do.\\n\\nThat’s why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce.\\n\\nLet’s get it done once and for all.\\n\\nAdvancing liberty and justice also requires protecting the rights of women.\\n\\nThe constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before.\\n\\nIf we want to go forward—not backward—we must protect access to health care. Preserve a woman’s right to choose. And let’s continue to advance maternal health care in America.\\n\\nAnd for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong.\\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.\\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.\\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things.\\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.\\n\\nFirst, beat the opioid epidemic.\\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.\\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers.\\n\\nIf you’re suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery.\\n\\nSecond, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.\\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.\\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor.\\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.\\n\\nAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit.\\n\\nIt’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children.\\n\\nAnd let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care.\\n\\nThird, support our veterans.\\n\\nVeterans are the best of us.\\n\\nI’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home.\\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.\\n\\nOur troops in Iraq and Afghanistan faced many dangers.\\n\\nOne was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='One was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more.\\n\\nWhen they came home, many of the world’s fittest and best trained warriors were never the same.\\n\\nHeadaches. Numbness. Dizziness.\\n\\nA cancer that would put them in a flag-draped coffin.\\n\\nI know.\\n\\nOne of those soldiers was my son Major Beau Biden.\\n\\nWe don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops.\\n\\nBut I’m committed to finding out everything we can.\\n\\nCommitted to military families like Danielle Robinson from Ohio.\\n\\nThe widow of Sergeant First Class Heath Robinson.\\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq.\\n\\nStationed near Baghdad, just yards from burn pits the size of football fields.\\n\\nHeath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter.\\n\\nBut cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body.\\n\\nDanielle says Heath was a fighter to the very end.\\n\\nHe didn’t know how to stop fighting, and neither did she.\\n\\nThrough her pain she found purpose to demand we do better.\\n\\nTonight, Danielle—we are.\\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits.\\n\\nAnd tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers.\\n\\nI’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve.\\n\\nAnd fourth, let’s end cancer as we know it.\\n\\nThis is personal to me and Jill, to Kamala, and to so many of you.\\n\\nCancer is the #2 cause of death in America–second only to heart disease.\\n\\nLast month, I announced our plan to supercharge the Cancer Moonshot that President Obama asked me to lead six years ago.\\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.\\n\\nMore support for patients and families.\\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health.\\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.\\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more.\\n\\nA unity agenda for the nation.\\n\\nWe can do this.\\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy.\\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things.\\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror.\\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known.\\n\\nNow is the hour.\\n\\nOur moment of responsibility.\\n\\nOur test of resolve and conscience, of history itself.\\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged.\\n\\nWell I know this nation.\\n\\nWe will meet the test.\\n\\nTo protect freedom and liberty, to expand fairness and opportunity.\\n\\nWe will save democracy.\\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life.\\n\\nBecause I see the future that is within our grasp.\\n\\nBecause I know there is simply nothing beyond our capacity.\\n\\nWe are the only nation on Earth that has always turned every crisis we have faced into an opportunity.\\n\\nThe only nation that can be defined by a single word: possibilities.\\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union.\\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong.\\n\\nWe are stronger today than we were a year ago.\\n\\nAnd we will be stronger a year from now than we are today.'),\n",
              " Document(metadata={'source': '/content/doc/state_of_the_union.txt'}, page_content='We are stronger today than we were a year ago.\\n\\nAnd we will be stronger a year from now than we are today.\\n\\nNow is our moment to meet and overcome the challenges of our time.\\n\\nAnd we will, as one people.\\n\\nOne America.\\n\\nThe United States of America.\\n\\nMay God bless you all. May God protect our troops.')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_astradb import AstraDBVectorStore\n",
        "from langchain.indexes import VectorstoreIndexCreator\n"
      ],
      "metadata": {
        "id": "aoQl_UBL00vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_API_ENDPOINT=\"https://451059f5-5cf6-4ff1-b16f-3c3c3e15e350-us-east-2.apps.astra.datastax.com\"\n",
        "ASTRA_DB_APPLICATION_TOKEN=\"AstraCS:oarzQcDZhpHjtEqSPjGaDpJu:a3ab8400ae8eadef3c3f3b16fd2399b865b5269b9f664a456256c5e28f304983\"\n",
        "ASTRA_DB_KEYSPACE=\"default_keyspace\""
      ],
      "metadata": {
        "id": "xmLZYizm2Zyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vstore = AstraDBVectorStore(\n",
        "    embedding=gemini_embeddings,\n",
        "    collection_name=\"multidoc_vector\",\n",
        "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
        "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
        "    namespace=ASTRA_DB_KEYSPACE,\n",
        ")"
      ],
      "metadata": {
        "id": "qGzS-2ki22n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inserted_ids = vstore.add_documents(docs)"
      ],
      "metadata": {
        "id": "AJXU71FJ2rDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_XqnBjj2922",
        "outputId": "fe91a457-f9d2-4da3-c8be-3104aa66c55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inserted 42 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an AI philosopher drawing insights from the roadmap of \"rag,\" \"llama3,\" and \"genai.\"\n",
        "Craft thoughtful answers based on this roadmap, mixing and matching existing paths.\n",
        "Your responses should be concise and strictly related to the provided context.\n",
        "\n",
        "ROADMAP CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "YOUR ANSWER:\"\"\""
      ],
      "metadata": {
        "id": "k3i_qaKyrgCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "TSgVNPOBrvpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "Q4qk9sXBr0XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vstore.as_retriever()"
      ],
      "metadata": {
        "id": "MrJfGwTvr2Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = retriever.invoke(\"what is a roadmap of generative AI\")"
      ],
      "metadata": {
        "id": "0_Z0AjgOsEn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIks5FwlthUj",
        "outputId": "0b824893-daf4-4d1a-e687-893af30eddcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(context)):\n",
        "  print(context[i].page_content)\n",
        "  print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8xjk0KStMP8",
        "outputId": "6239b357-0714-4832-caf1-e17f3aa67423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI\n",
            "\n",
            "Roadmap 2024\n",
            "\n",
            "From Basics to Advanced Concepts \n",
            "\n",
            "Here's a step-by-step guide designed for absolute beginners looking to acquire skills in Generative AI. \n",
            "\n",
            "The roadmap incorporates free learning resources for both technical and tool-related skills. \n",
            "\n",
            "Different Positions or Different Levels:  \n",
            "\n",
            "Developer Level 1 or Beginner Level \n",
            "\n",
            "Developer Level 2 or Senior Level\n",
            "\n",
            " Researcher Leve\n",
            "\n",
            "This is broken into many sections:\n",
            "\n",
            "Prerequisite\n",
            "\n",
            "Fundamentals\n",
            "\n",
            "Core Generative Models\n",
            "\n",
            "Developing Applications Powered by LLMs\n",
            "\n",
            "Projects and Practical Experience\n",
            "\n",
            "Miscellaneous Topics\n",
            "\n",
            "Advice for Productive Learning\n",
            "\n",
            "FAQs\n",
            "\n",
            "What is Generative AI?\n",
            "\n",
            "Generative AI generates new data based on training samples. Generative models can generate Image, Text, Audio, Videos etc. data as output.\n",
            "\n",
            "So generative AI is a very huge topic, \n",
            "\n",
            "Generative Image Model(GANs, Various Diffusions Models\n",
            "\n",
            "Generative Language Model(LLMs)\n",
            "\n",
            "When I refer to large language models, I mean natural language processing. Since NLP forms the foundation of massive language generated models(LLMs).\n",
            "\n",
            "Useful Learning Resource:\n",
            "\n",
            "Gen AI Introduction Video In English: DAY - 1 | Introduction to Generative AI Community Course LIVE ! #genai #ineuron - YouTube\n",
            "\n",
            "Gen AI Introduction Video In hindi: DAY - 1 | Introduction to Generative AI Community Course in HINDI LIVE ! #genai #ineuron - YouTube\n",
            "\n",
            "Gen AI Foundation Free Course : Sign In | iNeuron.ai\n",
            "\n",
            "GenAI Introduction by Krish Naik: Webinar - Generative AI Revolution : The Future | 2024 (youtube.com)\n",
            "\n",
            "Generative AI with Large Language Models:\n",
            "\n",
            "Prerequisite:\n",
            "\n",
            "Programming Language: \n",
            "\n",
            "Python is the most commonly used programming language for Data Science, Machine learning and Ai domain.\n",
            "\n",
            "Here are some reasons why:\n",
            "\n",
            "Community Support: Active community of developers, researchers, and practitioners in the machine learning and AI domains \n",
            "\n",
            "Libraries and Frameworks: Many key libraries and frameworks for generative AI, such as TensorFlow, PyTorch, and Keras, have Python interfaces \n",
            "\n",
            "Flexibility and Productivity: Python is known for its readability, simplicity, and ease of use, making it an ideal language for rapid prototyping and experimentation \n",
            "\n",
            "Data Analysis and Visualization: Python is widely used in data science, and it has excellent support for data manipulation and visualisation libraries, such as NumPy, Pandas, and Matplotlib.\n",
            "\n",
            "Topics to Learn- \n",
            "\n",
            "Variables, Numbers, Strings \n",
            "\n",
            "Lists, Dictionaries, Sets, tuples \n",
            "\n",
            "If condition, for loop\n",
            "\n",
            "Functions, Lambda Functions \n",
            "\n",
            "Modules (pip install)\n",
            "\n",
            "Read, Write files\n",
            "\n",
            "Exception handling \n",
            "\n",
            "Classes, Objects \n",
            "\n",
            "Useful Learning Resource: Sign In | iNeuron.ai\n",
            "\n",
            "SQL Database(Optional)\n",
            "\n",
            "Deep learning projects often involve working with large volumes of unstructured data, such as images, text, or audio. In any cases, traditional SQL databases may not be the primary choice for storing such unstructured data, and other types of data storage solutions might be more appropriate But for the machine learning project it is important to.\n",
            "\n",
            "My Preference: MySql or Sqlite3\n",
            "\n",
            "SQL Topics to Learn:\n",
            "\n",
            "Basics of Relational Databases \n",
            "\n",
            "Basic Queries: SELECT, WHERE LIKE, DISTINCT, BETWEEN, GROUP BY, ORDER BY\n",
            "\n",
            "Advanced Queries: CTE, Subqueries, Window Functions Joins: Left, Right, Inner, Full\n",
            "\n",
            "Useful Learning Resources\n",
            "\n",
            "SQL Complete Playlist - https://bit.ly/3vBFMFë \n",
            "\n",
            "NoSQL Database:\n",
            "\n",
            "The need for a NoSQL database in a deep learning project depends on the nature of your data and the specific requirements of your project. Deep learning projects often involve working with large volumes of unstructured data, such as images, text, or audio. In many cases, NoSQL databases are used to store and manage such unstructured data efficiently. \n",
            "\n",
            "Reason by you should Nosql Database: \n",
            "\n",
            "Scalability and Flexibility:\n",
            "\n",
            "Variety of Data Types:\n",
            "\n",
            "Real-time Data Ingestion:\n",
            "\n",
            "Distributed Computing:\n",
            "\n",
            "Schema-less Design: \n",
            "\n",
            "My Preference: MongoDB or CassandraDB \n",
            "\n",
            "Useful Learning Resource: \n",
            "\n",
            "https://www.youtube.com/watch?v=VIAcD6P_Etc\n",
            "==================================================\n",
            "https://aimagazine.com/\n",
            "\n",
            "Link of NeuroLAB: \n",
            "\n",
            "https://neurolab.ineuron.ai/\n",
            "\n",
            "FAQs\n",
            "\n",
            "Do I need a background in machine learning or deep learning to start learning generative AI?\n",
            "\n",
            "While a basic understanding of machine learning concepts is beneficial, some introductory courses in machine learning can help bridge the knowledge gap. Deep learning is a fundamental part of Generative AI, and a background in it is highly recommended. Familiarity with neural networks, backpropagation, RNNs and common deep learning frameworks like TensorFlow or [yTorch is advantageous for comprehending generative models.\n",
            "\n",
            "How much mathematics knowledge is required for generative AI?\n",
            "\n",
            "Mathematics is a key component of understanding the algorithms behind generative models. Students inquire about the level of mathematical knowledge needed, with a focus on linear algebra, calculus, and probability. A foundational understanding of these mathematical concepts is beneficial.\n",
            "\n",
            "Can I start with generative AI without prior experience in AI or computer science?\n",
            "\n",
            "Yes, you can start but a background in AI or computer science can provide a smoother start, there are beginner-friendly resources available to help newcomers build their skills. You can refer to this video for more details.\n",
            "\n",
            "Mastering Generative AI with OpenAI, LangChain, and LlamaIndex Batch Launch: Big Announcement 🚀 ! Mastering Generative AI with OpenAI, LangChain, and LlamaIndex Batch Launch 🌟 - YouTube\n",
            "==================================================\n",
            "3domain speciﬁc datasets\n",
            "\n",
            "(3)\n",
            "\n",
            "passage is pre-pended with the title of the research paper. Reconstruction Statement Generation: We use sen- tences from the abstract section of research articles for the reconstruction signal. We ﬁrst extract the abstract sections in 10K papers and split them into sentences using the NLTK library (Loper and Bird, 2002). We ﬁlter out the sentences that are too short (less than 15 words) or too long (more than 35 words). In this process, approximately 50,000 ab- stract statements are generated. It is important to note that when generating the knowledge base, we exclude the abstract sections. Synthetic QA Generation: In this domain, we only use synthetic data for training and validation. Fol- lowing the prior work (Shakeri et al., 2020), we use a BART seq2seq model trained on the SQuAD dataset (Rajpurkar et al., 2016) to generate syn- thetic QA pairs given a passage. We used the Squad dataset’s passages as the input and corresponding question-answer pairs as the expected output. We trained a BART-large checkpoint for two epochs. Then, we followed round-trip consistency (Alberti et al., 2019) to ﬁlter synthetic QA pairs. Our ﬁnal synthesized QA dataset consisted of 225,000 QA pairs. We use 90% of these QA pairs as training data and 10% as validation data. As the test data, we use 2000 human-labeled question-answer pairs from the COVID-QA dataset (Moller et al., 2020). News QA Domain Knowledge Base Generation: We extract 85,000 100-word passages as the knowledge base us- ing 10,000 news articles from the NewsQA dataset (Trischler et al., 2016). Reconstruction Statement Generation: We extract corresponding news summary sentences from the CNN/DM dataset (Hermann et al., 2015) for the re- construction signal. Every article consists of more than one summary sentence. However, we use the ﬁrst sentence as the title of the article, which we used in knowledge base generation and the rest of the statements as reconstruction statements. Our ﬁnal dataset contains 35,000 summary statements. QA Generation: The NewsQA dataset (Trischler et al., 2016) consists of 100,000 human anno- tated QA pairs from 10,000 news articles from the CNN/DM dataset (Hermann et al., 2015). We use the train (90,000), valid (5,000) and test (5,000) splits given in the dataset to train and evaluate our model. All questions in the NewsQA dataset focus on the high-level content of articles. So, to answer\n",
            "\n",
            "these questions, the model must access a large span of passages to conduct the reasoning process. Conversation QA Domain Knowledge Base Generation: We create the exter- nal knowledge base of 110,000 passages by split- ting the 10,000 conversations given in the QAConv dataset (Wu et al., 2021b) into passages, each with at most 100 words. We prepend the identiﬁer of each conversation (found in the original dataset) as the title of the passages. We also appended the speaker’s name, followed by the \":\" symbol, to the starting position of each dialogue to keep each conversation connected to its speakers. Reconstruction Statement Generation: We use the state-of-the-art abstractive conversation summa- rization model4 (Wu et al., 2021a) to generate one-sentence (TLDR) summary (approximately 45 words per conversation). We then use this as the auxiliary signal. We only generate sum- maries of conversations with more than 45 words. By doing this, we collect 35,000 synthetic sum- mary/reconstruction statements. QA Generation: We use the QAConv dataset (Wu et al., 2021b), which contains 35,000 QA pairs generated from 10,000 conversations that involved two or more parties. We use the train (25,000), valid (5,000) and test (5,000) splits given in the dataset to train and evaluate our model.\n",
            "\n",
            "4.2 Training and Evaluation Setup\n",
            "==================================================\n",
            "2 Background and Related Work\n",
            "\n",
            "Open-domain QA systems (Yang et al., 2015; Kwiatkowski et al., 2019) generally have a two- stage pipeline: passage retrieval (i.e., ﬁnding rele- vant text chunks related to an input question from a knowledge base) and machine comprehension (i.e., generating an answer from a set of selected documents). Traditionally sparse vector methods such as TF-IDF and BM25 are used for document retrieval (Robertson and Zaragoza, 2009). Re- searchers have recently moved to use dense text representations, which allows modeling textual sim- ilarity more semantic level. A recent example is the ‘Dense Passage Retriever (DPR)’ (Karpukhin et al., 2020), which generates embeddings for questions and text passages using two BERT (Devlin et al., 2018) models. The dot product of the embeddings is used as a similarity score between a question and a passage. DPR has demonstrated that higher retrieval precision results in a higher end-to-end QA accuracy. For the answer generation compo- nent of QA systems, recent studies have used either extractive language models like BERT or genera- tive language models like BART/GPT-2 (Min et al., 2021; Lewis et al., 2021).\n",
            "\n",
            "2.1 Retrieval Augmented Architecture\n",
            "\n",
            "Recently, Retrieval Augmented Architectures (Lewis et al., 2020b; Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain\n",
            "\n",
            "2Huggingface Transformers implementation\n",
            "\n",
            "QA architectures, RAG (Lewis et al., 2020b) com- bines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia arti- cles indexed with the FAISS library (Johnson et al., 2017). RAG ﬁrst encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can ﬁnetune both the generator and the question en- coder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG’s ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019). Other recent work also highlights how the outputs generated from RAG models are much more factual due to RAG being conditioned on the retrieved documents, possibly providing an an- swer to the hallucination problem of generative language models. Shuster, Kurt, et al. (Shuster et al., 2021) also highlight how RAG reduces hal- lucinations in knowledge-grounded conversational tasks, where the task is to generate responses to dialogues based on a large Wikipedia knowledge base. Xu et al. (2021) illustrate the effectiveness of RAG in chat-bot frameworks and highlight how RAG models are able to recall and summarize con- versations compared to standard seq2seq models with only parametric memory. This paper aims to understand how RAG could be extended to an end2end model and adapted to speciﬁc domains. To the best of our knowledge, this is the ﬁrst time RAG is being investigated on domain adaptation for the task of ODQA systems.\n",
            "\n",
            "2.2 REALM-like end2end Retrieval Augment\n",
            "\n",
            "Architectures\n",
            "\n",
            "REALM (Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that in- In the volves an end-to-end trainable retriever. REALM work, the authors ﬁrst train the entire model on the masked language prediction task and then ﬁne-tune it on question-answering tasks (keeping the retriever frozen). In comparison to REALM, the original RAG model uses an already trained DPR retriever and conducts partial end-to-\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an AI philosopher drawing insights from the roadmap of \"rag,\" \"llama3,\" and \"genai.\"\n",
        "Craft thoughtful answers based on this roadmap, mixing and matching existing paths.\n",
        "Your responses should be concise and strictly related to the provided context.\n",
        "\n",
        "ROADMAP CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "YOUR ANSWER:\"\"\""
      ],
      "metadata": {
        "id": "Y0L7uGVItO6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "BqJUNjqgt7Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Turn your prompt template string into a PromptTemplate object\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "_dPMh4eJt9lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"what is a roadmap of generative AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "xsL-zFwWucxV",
        "outputId": "52bd44e0-6856-44ab-f7b0-b6c8065d2438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A roadmap for generative AI, as described in the context, outlines a learning path from beginner to advanced levels. It encompasses the following:\\n\\n* **Foundation:** Understanding the basics of AI, machine learning, and deep learning concepts, along with essential programming (Python) and database skills (SQL, NoSQL). \\n* **Core Generative Models:** Delving into the architecture and functionality of generative models, specifically GANs, diffusion models for images, and LLMs for text.\\n* **Applications:**  Learning to develop applications that leverage the power of LLMs, like RAG and LlamaIndex, for tasks like question answering.\\n* **Projects:** Gaining practical experience by building projects and experimenting with different generative AI techniques. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}